{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8685/2110631893.py:19: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#Importando bibliotecas\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'UCI'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPC SSL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de Treino\n",
    "\n",
    "data_path = Path(f'/workspaces/betania.silva/view_concatenated/{DATASET_NAME}/train')\n",
    "\n",
    "datas_x_train = []\n",
    "\n",
    "data_y_train = []\n",
    "\n",
    "for f in data_path.glob('*.csv'):\n",
    "    data = pd.read_csv(f)\n",
    "    x = data[['accel-x', 'accel-y', 'accel-z', 'gyro-x', 'gyro-y', 'gyro-z']].values\n",
    "\n",
    "    # Expend dimension\n",
    "\n",
    "    x = np.swapaxes(x, 1, 0)\n",
    "    \n",
    "    datas_x_train.append(x)\n",
    "\n",
    "    y = data['standard activity code'].values\n",
    "\n",
    "    data_y_train.append(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de Teste\n",
    "\n",
    "data_path = Path(f'/workspaces/betania.silva/view_concatenated/{DATASET_NAME}/test')\n",
    "\n",
    "datas_x_test = []\n",
    "\n",
    "data_y_test = []\n",
    "\n",
    "for f in data_path.glob('*.csv'):\n",
    "    data = pd.read_csv(f)\n",
    "    x = data[['accel-x', 'accel-y', 'accel-z', 'gyro-x', 'gyro-y', 'gyro-z']].values\n",
    "\n",
    "    # Expend dimension\n",
    "\n",
    "    x = np.swapaxes(x, 1, 0)\n",
    "    \n",
    "    datas_x_test.append(x)\n",
    "\n",
    "    y = data['standard activity code'].values\n",
    "\n",
    "    data_y_test.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = datas_x_train\n",
    "y_train = data_y_train\n",
    "x_test = datas_x_test\n",
    "y_test = data_y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume todos os vetores z em em único vetor de contexto​\n",
    "\n",
    "class GRUEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 100,\n",
    "        in_channel: int = 561,\n",
    "        encoding_size: int = 10,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        bidirectional: bool = True,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_channel = in_channel\n",
    "        self.num_layers = num_layers\n",
    "        self.encoding_size = encoding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.device = device\n",
    "        \n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=self.in_channel,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "        ).to(device) \n",
    "\n",
    "        self.nn = torch.nn.Linear(\n",
    "            self.hidden_size * (int(self.bidirectional) + 1), self.encoding_size\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)\n",
    "        print(x.size())\n",
    "\n",
    "        past = torch.zeros(\n",
    "            self.num_layers * (int(self.bidirectional) + 1),\n",
    "            x.shape[1],\n",
    "            self.hidden_size,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        out, _ = self.rnn(\n",
    "            x, past\n",
    "        )  # out shape = [seq_len, batch_size, num_directions*hidden_size]\n",
    "        encodings = self.nn(out[-1].squeeze(0))\n",
    "\n",
    "        return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: torch.nn.Module,\n",
    "        density_estimator: torch.nn.Module,\n",
    "        auto_regressor: torch.nn.Module,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 0.0,\n",
    "        window_size: int = 4,\n",
    "        n_size: int = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.density_estimator = density_estimator.to(self.device)\n",
    "        self.auto_regressor = auto_regressor.to(self.device)\n",
    "        self.learning_rate = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.window_size = window_size\n",
    "        self.n_size = n_size\n",
    "        self.training_step_losses = []\n",
    "        self.best_train_loss = float('inf')\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        assert len(batch) == 1, \"Batch must be 1 sample only\"\n",
    "        sample = batch\n",
    "        sample = sample.squeeze(0)\n",
    "        rnd_t = np.random.randint(\n",
    "            5 * self.window_size, sample.shape[-1] - 5 * self.window_size\n",
    "        )\n",
    "        sample = torch.tensor(\n",
    "            sample[\n",
    "                :,\n",
    "                max(0, (rnd_t - 20 * self.window_size)) : min(\n",
    "                    sample.shape[-1], rnd_t + 20 * self.window_size\n",
    "                ),\n",
    "            ]\n",
    "        ).cpu()\n",
    "\n",
    "        T = sample.shape[-1]\n",
    "        windowed_sample = np.split(\n",
    "            sample[:, : (T // self.window_size) * self.window_size],\n",
    "            (T // self.window_size),\n",
    "            -1,\n",
    "        )\n",
    "        windowed_sample = torch.tensor(np.stack(windowed_sample, 0), device=self.device)\n",
    "        encodings = self.encoder(windowed_sample)\n",
    "        window_ind = torch.randint(2, len(encodings) - 2, size=(1,))\n",
    "        _, c_t = self.auto_regressor(\n",
    "            encodings[max(0, window_ind[0] - 10) : window_ind[0] + 1].unsqueeze(0)\n",
    "        )\n",
    "        density_ratios = torch.bmm(\n",
    "            encodings.unsqueeze(1),\n",
    "            self.density_estimator(c_t.squeeze(1).squeeze(0)).expand_as(encodings).unsqueeze(-1),\n",
    "        ).view(\n",
    "            -1,\n",
    "        )\n",
    "        r = set(range(0, window_ind[0] - 2))\n",
    "        r.update(set(range(window_ind[0] + 3, len(encodings))))\n",
    "        rnd_n = np.random.choice(list(r), self.n_size)\n",
    "        X_N = torch.cat(\n",
    "            [density_ratios[rnd_n], density_ratios[window_ind[0] + 1].unsqueeze(0)], 0\n",
    "        )\n",
    "        labels = torch.Tensor([len(X_N) - 1]).to(self.device)\n",
    "        loss = torch.nn.CrossEntropyLoss()(X_N.view(1, -1), labels.long())\n",
    "        if loss < self.best_train_loss:\n",
    "            self.best_train_loss = loss\n",
    "        self.training_step_losses.append(loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.training_step_losses).mean()\n",
    "        self.log(\"train_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        if epoch_mean < self.best_train_loss:\n",
    "            self.best_train_loss = epoch_mean\n",
    "            self.log(\"best_train_loss\", self.best_train_loss, on_epoch=True, on_step=False, prog_bar=False, logger=True)\n",
    "        # free up the memory\n",
    "        self.training_step_losses.clear()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        learnable_parameters = (\n",
    "            list(self.density_estimator.parameters())\n",
    "            + list(self.encoder.parameters())\n",
    "            + list(self.auto_regressor.parameters())\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            learnable_parameters, lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie uma instância do ModelCheckpoint para salvar nos pontos de verificação correspondentes à perda mínima\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filename='best_model',\n",
    "    monitor='train_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_last=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 6\n",
    "encoding_size = 10\n",
    "\n",
    "encoder = GRUEncoder(in_channel=6, encoding_size = 10, device='cpu')\n",
    "density_estimator = torch.nn.Linear(encoding_size, encoding_size)\n",
    "auto_regressor = torch.nn.GRU(\n",
    "    input_size=encoding_size, \n",
    "    hidden_size=encoding_size, \n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "cpc = CPC(encoder, density_estimator, auto_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleDataset:\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx].astype(np.float32), self.y[idx].astype(np.float32)\n",
    "        else:\n",
    "            return self.X[idx].astype(np.float32)\n",
    "    \n",
    "train_dataset = SimpleDataset(x_train)\n",
    "test_dataset = SimpleDataset(x_test, y_test)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/betania_meta4/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/home/betania_meta4/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=5, accelerator=\"cpu\", devices=1, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type       | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder           | GRUEncoder | 66.8 K\n",
      "1 | density_estimator | Linear     | 110   \n",
      "2 | auto_regressor    | GRU        | 660   \n",
      "-------------------------------------------------\n",
      "67.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "67.6 K    Total params\n",
      "0.270     Total estimated model params size (MB)\n",
      "/home/betania_meta4/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/21 [00:00<?, ?it/s] torch.Size([4, 40, 6])\n",
      "Epoch 0:   5%|▍         | 1/21 [00:00<00:00, 25.18it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  10%|▉         | 2/21 [00:00<00:00, 34.23it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  14%|█▍        | 3/21 [00:00<00:00, 37.71it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  19%|█▉        | 4/21 [00:00<00:00, 39.21it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  24%|██▍       | 5/21 [00:00<00:00, 41.34it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  29%|██▊       | 6/21 [00:00<00:00, 43.07it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  33%|███▎      | 7/21 [00:00<00:00, 44.35it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  38%|███▊      | 8/21 [00:00<00:00, 45.13it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  43%|████▎     | 9/21 [00:00<00:00, 46.53it/s, v_num=95]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8685/2124898713.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 40, 6])\n",
      "Epoch 0:  48%|████▊     | 10/21 [00:00<00:00, 47.73it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  52%|█████▏    | 11/21 [00:00<00:00, 49.23it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  57%|█████▋    | 12/21 [00:00<00:00, 48.58it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  62%|██████▏   | 13/21 [00:00<00:00, 49.27it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  67%|██████▋   | 14/21 [00:00<00:00, 50.39it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  71%|███████▏  | 15/21 [00:00<00:00, 51.17it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  76%|███████▌  | 16/21 [00:00<00:00, 52.21it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  81%|████████  | 17/21 [00:00<00:00, 52.83it/s, v_num=95]torch.Size([4, 30, 6])\n",
      "Epoch 0:  86%|████████▌ | 18/21 [00:00<00:00, 54.16it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  90%|█████████ | 19/21 [00:00<00:00, 54.85it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0:  95%|█████████▌| 20/21 [00:00<00:00, 54.80it/s, v_num=95]torch.Size([4, 40, 6])\n",
      "Epoch 0: 100%|██████████| 21/21 [00:00<00:00, 55.22it/s, v_num=95]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 21: 'train_loss' reached 1.80960 (best 1.80960), saving model to '/workspaces/betania.silva/ssl_tools/cpc/lightning_logs/version_95/checkpoints/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95]         torch.Size([4, 40, 6])\n",
      "Epoch 1:   5%|▍         | 1/21 [00:00<00:02,  7.73it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  10%|▉         | 2/21 [00:00<00:01, 13.36it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  14%|█▍        | 3/21 [00:00<00:00, 18.16it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  19%|█▉        | 4/21 [00:00<00:00, 22.04it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  24%|██▍       | 5/21 [00:00<00:00, 24.57it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  29%|██▊       | 6/21 [00:00<00:00, 26.94it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  33%|███▎      | 7/21 [00:00<00:00, 28.72it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  38%|███▊      | 8/21 [00:00<00:00, 30.51it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  43%|████▎     | 9/21 [00:00<00:00, 32.34it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  48%|████▊     | 10/21 [00:00<00:00, 33.96it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  52%|█████▏    | 11/21 [00:00<00:00, 35.29it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  57%|█████▋    | 12/21 [00:00<00:00, 36.57it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  62%|██████▏   | 13/21 [00:00<00:00, 37.69it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  67%|██████▋   | 14/21 [00:00<00:00, 38.73it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  71%|███████▏  | 15/21 [00:00<00:00, 39.50it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  76%|███████▌  | 16/21 [00:00<00:00, 40.07it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  81%|████████  | 17/21 [00:00<00:00, 40.80it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  86%|████████▌ | 18/21 [00:00<00:00, 41.31it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  90%|█████████ | 19/21 [00:00<00:00, 41.86it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1:  95%|█████████▌| 20/21 [00:00<00:00, 42.36it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 1: 100%|██████████| 21/21 [00:00<00:00, 42.88it/s, v_num=95, train_loss=1.810]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 42: 'train_loss' reached 1.80574 (best 1.80574), saving model to '/workspaces/betania.silva/ssl_tools/cpc/lightning_logs/version_95/checkpoints/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.810]         torch.Size([4, 40, 6])\n",
      "Epoch 2:   5%|▍         | 1/21 [00:00<00:02,  7.05it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  10%|▉         | 2/21 [00:00<00:01, 12.21it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  14%|█▍        | 3/21 [00:00<00:01, 16.32it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  19%|█▉        | 4/21 [00:00<00:00, 19.48it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  24%|██▍       | 5/21 [00:00<00:00, 22.24it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  29%|██▊       | 6/21 [00:00<00:00, 24.48it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  33%|███▎      | 7/21 [00:00<00:00, 26.52it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  38%|███▊      | 8/21 [00:00<00:00, 28.55it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  43%|████▎     | 9/21 [00:00<00:00, 29.93it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  48%|████▊     | 10/21 [00:00<00:00, 31.27it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  52%|█████▏    | 11/21 [00:00<00:00, 32.46it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  57%|█████▋    | 12/21 [00:00<00:00, 33.43it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  62%|██████▏   | 13/21 [00:00<00:00, 34.37it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  67%|██████▋   | 14/21 [00:00<00:00, 35.23it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  71%|███████▏  | 15/21 [00:00<00:00, 36.15it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  76%|███████▌  | 16/21 [00:00<00:00, 37.02it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  81%|████████  | 17/21 [00:00<00:00, 37.75it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  86%|████████▌ | 18/21 [00:00<00:00, 38.50it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  90%|█████████ | 19/21 [00:00<00:00, 39.26it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2:  95%|█████████▌| 20/21 [00:00<00:00, 40.03it/s, v_num=95, train_loss=1.810]torch.Size([4, 40, 6])\n",
      "Epoch 2: 100%|██████████| 21/21 [00:00<00:00, 40.47it/s, v_num=95, train_loss=1.810]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 63: 'train_loss' reached 1.78472 (best 1.78472), saving model to '/workspaces/betania.silva/ssl_tools/cpc/lightning_logs/version_95/checkpoints/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.810]         torch.Size([4, 40, 6])\n",
      "Epoch 3:   5%|▍         | 1/21 [00:00<00:02,  7.13it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  10%|▉         | 2/21 [00:00<00:01, 12.35it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  14%|█▍        | 3/21 [00:00<00:01, 15.81it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  19%|█▉        | 4/21 [00:00<00:00, 19.45it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  24%|██▍       | 5/21 [00:00<00:00, 22.34it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  29%|██▊       | 6/21 [00:00<00:00, 24.82it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  33%|███▎      | 7/21 [00:00<00:00, 26.89it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  38%|███▊      | 8/21 [00:00<00:00, 28.75it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  43%|████▎     | 9/21 [00:00<00:00, 30.37it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  48%|████▊     | 10/21 [00:00<00:00, 31.65it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  52%|█████▏    | 11/21 [00:00<00:00, 33.15it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  57%|█████▋    | 12/21 [00:00<00:00, 34.53it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  62%|██████▏   | 13/21 [00:00<00:00, 35.73it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  67%|██████▋   | 14/21 [00:00<00:00, 36.82it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  71%|███████▏  | 15/21 [00:00<00:00, 37.47it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  76%|███████▌  | 16/21 [00:00<00:00, 38.06it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  81%|████████  | 17/21 [00:00<00:00, 38.61it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  86%|████████▌ | 18/21 [00:00<00:00, 39.04it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  90%|█████████ | 19/21 [00:00<00:00, 39.47it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3:  95%|█████████▌| 20/21 [00:00<00:00, 39.92it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 3: 100%|██████████| 21/21 [00:00<00:00, 40.21it/s, v_num=95, train_loss=1.780]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 84: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.780]         torch.Size([4, 40, 6])\n",
      "Epoch 4:   5%|▍         | 1/21 [00:00<00:03,  5.81it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  10%|▉         | 2/21 [00:00<00:01, 10.28it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  14%|█▍        | 3/21 [00:00<00:01, 13.92it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  19%|█▉        | 4/21 [00:00<00:01, 16.84it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  24%|██▍       | 5/21 [00:00<00:00, 19.30it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  29%|██▊       | 6/21 [00:00<00:00, 21.41it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  33%|███▎      | 7/21 [00:00<00:00, 23.20it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  38%|███▊      | 8/21 [00:00<00:00, 24.79it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  43%|████▎     | 9/21 [00:00<00:00, 26.27it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  48%|████▊     | 10/21 [00:00<00:00, 27.47it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  52%|█████▏    | 11/21 [00:00<00:00, 28.72it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  57%|█████▋    | 12/21 [00:00<00:00, 29.82it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  62%|██████▏   | 13/21 [00:00<00:00, 30.92it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  67%|██████▋   | 14/21 [00:00<00:00, 31.75it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  71%|███████▏  | 15/21 [00:00<00:00, 32.66it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  76%|███████▌  | 16/21 [00:00<00:00, 33.29it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  81%|████████  | 17/21 [00:00<00:00, 34.04it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  86%|████████▌ | 18/21 [00:00<00:00, 34.69it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  90%|█████████ | 19/21 [00:00<00:00, 35.44it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4:  95%|█████████▌| 20/21 [00:00<00:00, 36.04it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 4: 100%|██████████| 21/21 [00:00<00:00, 36.67it/s, v_num=95, train_loss=1.800]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 105: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.800]         torch.Size([4, 40, 6])\n",
      "Epoch 5:   5%|▍         | 1/21 [00:00<00:03,  6.02it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  10%|▉         | 2/21 [00:00<00:01, 10.88it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  14%|█▍        | 3/21 [00:00<00:01, 14.72it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  19%|█▉        | 4/21 [00:00<00:00, 17.72it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  24%|██▍       | 5/21 [00:00<00:00, 20.25it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  29%|██▊       | 6/21 [00:00<00:00, 22.69it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  33%|███▎      | 7/21 [00:00<00:00, 24.71it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  38%|███▊      | 8/21 [00:00<00:00, 26.48it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  43%|████▎     | 9/21 [00:00<00:00, 28.06it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  48%|████▊     | 10/21 [00:00<00:00, 29.41it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  52%|█████▏    | 11/21 [00:00<00:00, 30.73it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  57%|█████▋    | 12/21 [00:00<00:00, 31.82it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  62%|██████▏   | 13/21 [00:00<00:00, 32.84it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  67%|██████▋   | 14/21 [00:00<00:00, 33.80it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  71%|███████▏  | 15/21 [00:00<00:00, 34.68it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  76%|███████▌  | 16/21 [00:00<00:00, 35.49it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  81%|████████  | 17/21 [00:00<00:00, 36.29it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  86%|████████▌ | 18/21 [00:00<00:00, 36.93it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  90%|█████████ | 19/21 [00:00<00:00, 37.60it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5:  95%|█████████▌| 20/21 [00:00<00:00, 38.22it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 5: 100%|██████████| 21/21 [00:00<00:00, 38.56it/s, v_num=95, train_loss=1.800]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 126: 'train_loss' reached 1.77704 (best 1.77704), saving model to '/workspaces/betania.silva/ssl_tools/cpc/lightning_logs/version_95/checkpoints/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.800]         torch.Size([4, 40, 6])\n",
      "Epoch 6:   5%|▍         | 1/21 [00:00<00:03,  6.42it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  10%|▉         | 2/21 [00:00<00:01, 11.17it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  14%|█▍        | 3/21 [00:00<00:01, 14.86it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  19%|█▉        | 4/21 [00:00<00:00, 17.86it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  24%|██▍       | 5/21 [00:00<00:00, 20.48it/s, v_num=95, train_loss=1.780]torch.Size([4, 33, 6])\n",
      "Epoch 6:  29%|██▊       | 6/21 [00:00<00:00, 22.97it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  33%|███▎      | 7/21 [00:00<00:00, 24.95it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  38%|███▊      | 8/21 [00:00<00:00, 26.59it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  43%|████▎     | 9/21 [00:00<00:00, 28.11it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  48%|████▊     | 10/21 [00:00<00:00, 29.33it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  52%|█████▏    | 11/21 [00:00<00:00, 30.51it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  57%|█████▋    | 12/21 [00:00<00:00, 31.57it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  62%|██████▏   | 13/21 [00:00<00:00, 32.53it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  67%|██████▋   | 14/21 [00:00<00:00, 33.44it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  71%|███████▏  | 15/21 [00:00<00:00, 34.20it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  76%|███████▌  | 16/21 [00:00<00:00, 35.03it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  81%|████████  | 17/21 [00:00<00:00, 35.66it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  86%|████████▌ | 18/21 [00:00<00:00, 36.29it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  90%|█████████ | 19/21 [00:00<00:00, 36.89it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6:  95%|█████████▌| 20/21 [00:00<00:00, 37.36it/s, v_num=95, train_loss=1.780]torch.Size([4, 40, 6])\n",
      "Epoch 6: 100%|██████████| 21/21 [00:00<00:00, 37.77it/s, v_num=95, train_loss=1.780]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 147: 'train_loss' reached 1.70045 (best 1.70045), saving model to '/workspaces/betania.silva/ssl_tools/cpc/lightning_logs/version_95/checkpoints/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.780]         torch.Size([4, 40, 6])\n",
      "Epoch 7:   5%|▍         | 1/21 [00:00<00:03,  6.54it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  10%|▉         | 2/21 [00:00<00:01, 11.38it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  14%|█▍        | 3/21 [00:00<00:01, 15.17it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  19%|█▉        | 4/21 [00:00<00:00, 18.21it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  24%|██▍       | 5/21 [00:00<00:00, 20.80it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  29%|██▊       | 6/21 [00:00<00:00, 23.07it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  33%|███▎      | 7/21 [00:00<00:00, 24.86it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  38%|███▊      | 8/21 [00:00<00:00, 26.30it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  43%|████▎     | 9/21 [00:00<00:00, 27.74it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  48%|████▊     | 10/21 [00:00<00:00, 28.78it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  52%|█████▏    | 11/21 [00:00<00:00, 30.12it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  57%|█████▋    | 12/21 [00:00<00:00, 31.14it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  62%|██████▏   | 13/21 [00:00<00:00, 32.34it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  67%|██████▋   | 14/21 [00:00<00:00, 33.12it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  71%|███████▏  | 15/21 [00:00<00:00, 34.14it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  76%|███████▌  | 16/21 [00:00<00:00, 35.00it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  81%|████████  | 17/21 [00:00<00:00, 35.80it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  86%|████████▌ | 18/21 [00:00<00:00, 36.53it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  90%|█████████ | 19/21 [00:00<00:00, 37.25it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7:  95%|█████████▌| 20/21 [00:00<00:00, 37.76it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 7: 100%|██████████| 21/21 [00:00<00:00, 38.25it/s, v_num=95, train_loss=1.700]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 168: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.700]         torch.Size([4, 40, 6])\n",
      "Epoch 8:   5%|▍         | 1/21 [00:00<00:03,  5.58it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  10%|▉         | 2/21 [00:00<00:01,  9.79it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  14%|█▍        | 3/21 [00:00<00:01, 13.36it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  19%|█▉        | 4/21 [00:00<00:01, 16.09it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  24%|██▍       | 5/21 [00:00<00:00, 18.58it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  29%|██▊       | 6/21 [00:00<00:00, 20.73it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  33%|███▎      | 7/21 [00:00<00:00, 22.34it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  38%|███▊      | 8/21 [00:00<00:00, 23.94it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  43%|████▎     | 9/21 [00:00<00:00, 25.34it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  48%|████▊     | 10/21 [00:00<00:00, 26.73it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  52%|█████▏    | 11/21 [00:00<00:00, 27.90it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  57%|█████▋    | 12/21 [00:00<00:00, 28.97it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  62%|██████▏   | 13/21 [00:00<00:00, 29.72it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  67%|██████▋   | 14/21 [00:00<00:00, 30.44it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  71%|███████▏  | 15/21 [00:00<00:00, 31.37it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  76%|███████▌  | 16/21 [00:00<00:00, 31.98it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  81%|████████  | 17/21 [00:00<00:00, 32.81it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  86%|████████▌ | 18/21 [00:00<00:00, 33.37it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  90%|█████████ | 19/21 [00:00<00:00, 34.05it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8:  95%|█████████▌| 20/21 [00:00<00:00, 34.57it/s, v_num=95, train_loss=1.800]torch.Size([4, 40, 6])\n",
      "Epoch 8: 100%|██████████| 21/21 [00:00<00:00, 35.08it/s, v_num=95, train_loss=1.800]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 189: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.800]         torch.Size([4, 40, 6])\n",
      "Epoch 9:   5%|▍         | 1/21 [00:00<00:03,  5.72it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  10%|▉         | 2/21 [00:00<00:01, 10.29it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  14%|█▍        | 3/21 [00:00<00:01, 13.96it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  19%|█▉        | 4/21 [00:00<00:01, 16.85it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  24%|██▍       | 5/21 [00:00<00:00, 19.36it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  29%|██▊       | 6/21 [00:00<00:00, 21.63it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  33%|███▎      | 7/21 [00:00<00:00, 23.59it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  38%|███▊      | 8/21 [00:00<00:00, 25.33it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  43%|████▎     | 9/21 [00:00<00:00, 26.95it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  48%|████▊     | 10/21 [00:00<00:00, 28.38it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  52%|█████▏    | 11/21 [00:00<00:00, 29.49it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  57%|█████▋    | 12/21 [00:00<00:00, 30.63it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  62%|██████▏   | 13/21 [00:00<00:00, 31.51it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  67%|██████▋   | 14/21 [00:00<00:00, 32.38it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  71%|███████▏  | 15/21 [00:00<00:00, 33.19it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  76%|███████▌  | 16/21 [00:00<00:00, 33.89it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  81%|████████  | 17/21 [00:00<00:00, 34.55it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  86%|████████▌ | 18/21 [00:00<00:00, 35.16it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  90%|█████████ | 19/21 [00:00<00:00, 35.87it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9:  95%|█████████▌| 20/21 [00:00<00:00, 36.48it/s, v_num=95, train_loss=2.000]torch.Size([4, 40, 6])\n",
      "Epoch 9: 100%|██████████| 21/21 [00:00<00:00, 36.88it/s, v_num=95, train_loss=2.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 210: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=2.000]        torch.Size([4, 40, 6])\n",
      "Epoch 10:   5%|▍         | 1/21 [00:00<00:03,  5.93it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  10%|▉         | 2/21 [00:00<00:01, 10.32it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  14%|█▍        | 3/21 [00:00<00:01, 14.05it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  19%|█▉        | 4/21 [00:00<00:00, 17.17it/s, v_num=95, train_loss=1.700]torch.Size([4, 36, 6])\n",
      "Epoch 10:  24%|██▍       | 5/21 [00:00<00:00, 19.17it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  29%|██▊       | 6/21 [00:00<00:00, 21.21it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  33%|███▎      | 7/21 [00:00<00:00, 23.35it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  38%|███▊      | 8/21 [00:00<00:00, 25.18it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  43%|████▎     | 9/21 [00:00<00:00, 26.69it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  48%|████▊     | 10/21 [00:00<00:00, 28.06it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  52%|█████▏    | 11/21 [00:00<00:00, 29.24it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  57%|█████▋    | 12/21 [00:00<00:00, 30.27it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  62%|██████▏   | 13/21 [00:00<00:00, 31.48it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  67%|██████▋   | 14/21 [00:00<00:00, 32.35it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  71%|███████▏  | 15/21 [00:00<00:00, 33.13it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  76%|███████▌  | 16/21 [00:00<00:00, 33.80it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  81%|████████  | 17/21 [00:00<00:00, 34.42it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  86%|████████▌ | 18/21 [00:00<00:00, 35.11it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  90%|█████████ | 19/21 [00:00<00:00, 35.77it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10:  95%|█████████▌| 20/21 [00:00<00:00, 36.39it/s, v_num=95, train_loss=1.700]torch.Size([4, 40, 6])\n",
      "Epoch 10: 100%|██████████| 21/21 [00:00<00:00, 36.88it/s, v_num=95, train_loss=1.700]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 231: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.700]         torch.Size([4, 40, 6])\n",
      "Epoch 11:   5%|▍         | 1/21 [00:00<00:03,  5.80it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  10%|▉         | 2/21 [00:00<00:01, 10.27it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  14%|█▍        | 3/21 [00:00<00:01, 13.95it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  19%|█▉        | 4/21 [00:00<00:01, 16.51it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  24%|██▍       | 5/21 [00:00<00:00, 19.06it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  29%|██▊       | 6/21 [00:00<00:00, 21.19it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  33%|███▎      | 7/21 [00:00<00:00, 23.16it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  38%|███▊      | 8/21 [00:00<00:00, 24.96it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  43%|████▎     | 9/21 [00:00<00:00, 26.47it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  48%|████▊     | 10/21 [00:00<00:00, 27.76it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  52%|█████▏    | 11/21 [00:00<00:00, 29.19it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  57%|█████▋    | 12/21 [00:00<00:00, 30.56it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  62%|██████▏   | 13/21 [00:00<00:00, 31.83it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  67%|██████▋   | 14/21 [00:00<00:00, 32.67it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  71%|███████▏  | 15/21 [00:00<00:00, 33.47it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  76%|███████▌  | 16/21 [00:00<00:00, 34.12it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  81%|████████  | 17/21 [00:00<00:00, 34.89it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  86%|████████▌ | 18/21 [00:00<00:00, 35.40it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  90%|█████████ | 19/21 [00:00<00:00, 35.93it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11:  95%|█████████▌| 20/21 [00:00<00:00, 36.37it/s, v_num=95, train_loss=1.710]torch.Size([4, 40, 6])\n",
      "Epoch 11: 100%|██████████| 21/21 [00:00<00:00, 36.78it/s, v_num=95, train_loss=1.710]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 252: 'train_loss' reached 1.59327 (best 1.59327), saving model to '/workspaces/betania.silva/ssl_tools/cpc/lightning_logs/version_95/checkpoints/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.710]         torch.Size([4, 40, 6])\n",
      "Epoch 12:   5%|▍         | 1/21 [00:00<00:03,  6.65it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  10%|▉         | 2/21 [00:00<00:01, 11.69it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  14%|█▍        | 3/21 [00:00<00:01, 15.77it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  19%|█▉        | 4/21 [00:00<00:00, 19.38it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  24%|██▍       | 5/21 [00:00<00:00, 22.23it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  29%|██▊       | 6/21 [00:00<00:00, 24.78it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  33%|███▎      | 7/21 [00:00<00:00, 27.13it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  38%|███▊      | 8/21 [00:00<00:00, 29.19it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  43%|████▎     | 9/21 [00:00<00:00, 30.65it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  48%|████▊     | 10/21 [00:00<00:00, 32.29it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  52%|█████▏    | 11/21 [00:00<00:00, 33.30it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  57%|█████▋    | 12/21 [00:00<00:00, 34.31it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  62%|██████▏   | 13/21 [00:00<00:00, 35.22it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  67%|██████▋   | 14/21 [00:00<00:00, 35.86it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  71%|███████▏  | 15/21 [00:00<00:00, 36.57it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  76%|███████▌  | 16/21 [00:00<00:00, 37.23it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  81%|████████  | 17/21 [00:00<00:00, 37.77it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  86%|████████▌ | 18/21 [00:00<00:00, 38.31it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  90%|█████████ | 19/21 [00:00<00:00, 39.01it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12:  95%|█████████▌| 20/21 [00:00<00:00, 39.36it/s, v_num=95, train_loss=1.590]torch.Size([4, 40, 6])\n",
      "Epoch 12: 100%|██████████| 21/21 [00:00<00:00, 39.79it/s, v_num=95, train_loss=1.590]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 273: 'train_loss' reached 1.50981 (best 1.50981), saving model to '/workspaces/betania.silva/ssl_tools/cpc/lightning_logs/version_95/checkpoints/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.590]         torch.Size([4, 40, 6])\n",
      "Epoch 13:   5%|▍         | 1/21 [00:00<00:02,  6.82it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  10%|▉         | 2/21 [00:00<00:01, 11.65it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  14%|█▍        | 3/21 [00:00<00:01, 15.57it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  19%|█▉        | 4/21 [00:00<00:00, 18.65it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  24%|██▍       | 5/21 [00:00<00:00, 21.52it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  29%|██▊       | 6/21 [00:00<00:00, 24.06it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  33%|███▎      | 7/21 [00:00<00:00, 25.94it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  38%|███▊      | 8/21 [00:00<00:00, 27.61it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  43%|████▎     | 9/21 [00:00<00:00, 29.20it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  48%|████▊     | 10/21 [00:00<00:00, 30.42it/s, v_num=95, train_loss=1.510]torch.Size([4, 33, 6])\n",
      "Epoch 13:  52%|█████▏    | 11/21 [00:00<00:00, 32.16it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  57%|█████▋    | 12/21 [00:00<00:00, 33.49it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  62%|██████▏   | 13/21 [00:00<00:00, 34.46it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  67%|██████▋   | 14/21 [00:00<00:00, 35.45it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  71%|███████▏  | 15/21 [00:00<00:00, 36.36it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  76%|███████▌  | 16/21 [00:00<00:00, 37.13it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  81%|████████  | 17/21 [00:00<00:00, 37.78it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  86%|████████▌ | 18/21 [00:00<00:00, 38.42it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  90%|█████████ | 19/21 [00:00<00:00, 39.07it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13:  95%|█████████▌| 20/21 [00:00<00:00, 39.62it/s, v_num=95, train_loss=1.510]torch.Size([4, 40, 6])\n",
      "Epoch 13: 100%|██████████| 21/21 [00:00<00:00, 39.93it/s, v_num=95, train_loss=1.510]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 294: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.510]         torch.Size([4, 40, 6])\n",
      "Epoch 14:   5%|▍         | 1/21 [00:00<00:03,  5.60it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  10%|▉         | 2/21 [00:00<00:01, 10.06it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  14%|█▍        | 3/21 [00:00<00:01, 13.67it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  19%|█▉        | 4/21 [00:00<00:01, 16.68it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  24%|██▍       | 5/21 [00:00<00:00, 19.29it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  29%|██▊       | 6/21 [00:00<00:00, 21.66it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  33%|███▎      | 7/21 [00:00<00:00, 23.63it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  38%|███▊      | 8/21 [00:00<00:00, 25.07it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  43%|████▎     | 9/21 [00:00<00:00, 26.37it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  48%|████▊     | 10/21 [00:00<00:00, 27.57it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  52%|█████▏    | 11/21 [00:00<00:00, 28.70it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  57%|█████▋    | 12/21 [00:00<00:00, 29.88it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  62%|██████▏   | 13/21 [00:00<00:00, 30.99it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  67%|██████▋   | 14/21 [00:00<00:00, 31.84it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  71%|███████▏  | 15/21 [00:00<00:00, 32.61it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  76%|███████▌  | 16/21 [00:00<00:00, 33.27it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  81%|████████  | 17/21 [00:00<00:00, 33.97it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  86%|████████▌ | 18/21 [00:00<00:00, 34.64it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  90%|█████████ | 19/21 [00:00<00:00, 35.33it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14:  95%|█████████▌| 20/21 [00:00<00:00, 35.72it/s, v_num=95, train_loss=1.600]torch.Size([4, 40, 6])\n",
      "Epoch 14: 100%|██████████| 21/21 [00:00<00:00, 36.04it/s, v_num=95, train_loss=1.600]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 315: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.600]         torch.Size([4, 40, 6])\n",
      "Epoch 15:   5%|▍         | 1/21 [00:00<00:03,  6.39it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  10%|▉         | 2/21 [00:00<00:01, 11.26it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  14%|█▍        | 3/21 [00:00<00:01, 15.31it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  19%|█▉        | 4/21 [00:00<00:00, 18.57it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  24%|██▍       | 5/21 [00:00<00:00, 21.23it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  29%|██▊       | 6/21 [00:00<00:00, 23.52it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  33%|███▎      | 7/21 [00:00<00:00, 25.44it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  38%|███▊      | 8/21 [00:00<00:00, 27.14it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  43%|████▎     | 9/21 [00:00<00:00, 28.58it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  48%|████▊     | 10/21 [00:00<00:00, 29.86it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  52%|█████▏    | 11/21 [00:00<00:00, 31.11it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  57%|█████▋    | 12/21 [00:00<00:00, 32.18it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  62%|██████▏   | 13/21 [00:00<00:00, 33.08it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  67%|██████▋   | 14/21 [00:00<00:00, 33.74it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  71%|███████▏  | 15/21 [00:00<00:00, 34.46it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  76%|███████▌  | 16/21 [00:00<00:00, 35.15it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  81%|████████  | 17/21 [00:00<00:00, 35.80it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  86%|████████▌ | 18/21 [00:00<00:00, 36.43it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  90%|█████████ | 19/21 [00:00<00:00, 36.99it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15:  95%|█████████▌| 20/21 [00:00<00:00, 37.50it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 15: 100%|██████████| 21/21 [00:00<00:00, 37.91it/s, v_num=95, train_loss=1.630]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 336: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.630]         torch.Size([4, 40, 6])\n",
      "Epoch 16:   5%|▍         | 1/21 [00:00<00:03,  6.60it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  10%|▉         | 2/21 [00:00<00:01, 11.47it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  14%|█▍        | 3/21 [00:00<00:01, 12.18it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  19%|█▉        | 4/21 [00:00<00:01, 14.81it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  24%|██▍       | 5/21 [00:00<00:00, 17.30it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  29%|██▊       | 6/21 [00:00<00:00, 19.41it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  33%|███▎      | 7/21 [00:00<00:00, 21.44it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  38%|███▊      | 8/21 [00:00<00:00, 23.17it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  43%|████▎     | 9/21 [00:00<00:00, 24.64it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  48%|████▊     | 10/21 [00:00<00:00, 25.96it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  52%|█████▏    | 11/21 [00:00<00:00, 27.18it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  57%|█████▋    | 12/21 [00:00<00:00, 28.28it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  62%|██████▏   | 13/21 [00:00<00:00, 29.28it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  67%|██████▋   | 14/21 [00:00<00:00, 30.20it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  71%|███████▏  | 15/21 [00:00<00:00, 31.01it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  76%|███████▌  | 16/21 [00:00<00:00, 31.85it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  81%|████████  | 17/21 [00:00<00:00, 32.58it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  86%|████████▌ | 18/21 [00:00<00:00, 33.27it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  90%|█████████ | 19/21 [00:00<00:00, 33.96it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16:  95%|█████████▌| 20/21 [00:00<00:00, 34.48it/s, v_num=95, train_loss=1.740]torch.Size([4, 40, 6])\n",
      "Epoch 16: 100%|██████████| 21/21 [00:00<00:00, 34.98it/s, v_num=95, train_loss=1.740]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 357: 'train_loss' reached 1.50360 (best 1.50360), saving model to '/workspaces/betania.silva/ssl_tools/cpc/lightning_logs/version_95/checkpoints/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.740]         torch.Size([4, 40, 6])\n",
      "Epoch 17:   5%|▍         | 1/21 [00:00<00:03,  6.51it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  10%|▉         | 2/21 [00:00<00:01, 11.39it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  14%|█▍        | 3/21 [00:00<00:01, 15.38it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  19%|█▉        | 4/21 [00:00<00:00, 18.81it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  24%|██▍       | 5/21 [00:00<00:00, 21.28it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  29%|██▊       | 6/21 [00:00<00:00, 23.69it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  33%|███▎      | 7/21 [00:00<00:00, 25.61it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  38%|███▊      | 8/21 [00:00<00:00, 27.40it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  43%|████▎     | 9/21 [00:00<00:00, 28.78it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  48%|████▊     | 10/21 [00:00<00:00, 30.01it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  52%|█████▏    | 11/21 [00:00<00:00, 31.15it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  57%|█████▋    | 12/21 [00:00<00:00, 32.24it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  62%|██████▏   | 13/21 [00:00<00:00, 33.19it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  67%|██████▋   | 14/21 [00:00<00:00, 34.06it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  71%|███████▏  | 15/21 [00:00<00:00, 34.75it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  76%|███████▌  | 16/21 [00:00<00:00, 35.37it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  81%|████████  | 17/21 [00:00<00:00, 35.93it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  86%|████████▌ | 18/21 [00:00<00:00, 36.63it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  90%|█████████ | 19/21 [00:00<00:00, 37.33it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17:  95%|█████████▌| 20/21 [00:00<00:00, 37.89it/s, v_num=95, train_loss=1.500]torch.Size([4, 40, 6])\n",
      "Epoch 17: 100%|██████████| 21/21 [00:00<00:00, 38.21it/s, v_num=95, train_loss=1.500]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 378: 'train_loss' reached 1.41476 (best 1.41476), saving model to '/workspaces/betania.silva/ssl_tools/cpc/lightning_logs/version_95/checkpoints/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.500]         torch.Size([4, 40, 6])\n",
      "Epoch 18:   5%|▍         | 1/21 [00:00<00:03,  6.63it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  10%|▉         | 2/21 [00:00<00:01, 11.50it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  14%|█▍        | 3/21 [00:00<00:01, 15.67it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  19%|█▉        | 4/21 [00:00<00:00, 19.08it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  24%|██▍       | 5/21 [00:00<00:00, 21.99it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  29%|██▊       | 6/21 [00:00<00:00, 24.47it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  33%|███▎      | 7/21 [00:00<00:00, 26.53it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  38%|███▊      | 8/21 [00:00<00:00, 28.61it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  43%|████▎     | 9/21 [00:00<00:00, 30.38it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  48%|████▊     | 10/21 [00:00<00:00, 31.59it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  52%|█████▏    | 11/21 [00:00<00:00, 33.12it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  57%|█████▋    | 12/21 [00:00<00:00, 34.54it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  62%|██████▏   | 13/21 [00:00<00:00, 35.78it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  67%|██████▋   | 14/21 [00:00<00:00, 36.91it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  71%|███████▏  | 15/21 [00:00<00:00, 37.97it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  76%|███████▌  | 16/21 [00:00<00:00, 38.85it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  81%|████████  | 17/21 [00:00<00:00, 39.51it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  86%|████████▌ | 18/21 [00:00<00:00, 40.17it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  90%|█████████ | 19/21 [00:00<00:00, 40.86it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18:  95%|█████████▌| 20/21 [00:00<00:00, 41.37it/s, v_num=95, train_loss=1.410]torch.Size([4, 40, 6])\n",
      "Epoch 18: 100%|██████████| 21/21 [00:00<00:00, 41.58it/s, v_num=95, train_loss=1.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 399: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.410]         torch.Size([4, 40, 6])\n",
      "Epoch 19:   5%|▍         | 1/21 [00:00<00:03,  6.66it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  10%|▉         | 2/21 [00:00<00:01, 11.62it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  14%|█▍        | 3/21 [00:00<00:01, 15.58it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  19%|█▉        | 4/21 [00:00<00:00, 18.61it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  24%|██▍       | 5/21 [00:00<00:00, 21.36it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  29%|██▊       | 6/21 [00:00<00:00, 23.74it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  33%|███▎      | 7/21 [00:00<00:00, 25.87it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  38%|███▊      | 8/21 [00:00<00:00, 27.59it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  43%|████▎     | 9/21 [00:00<00:00, 29.06it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  48%|████▊     | 10/21 [00:00<00:00, 30.33it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  52%|█████▏    | 11/21 [00:00<00:00, 31.81it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  57%|█████▋    | 12/21 [00:00<00:00, 33.11it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  62%|██████▏   | 13/21 [00:00<00:00, 34.23it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  67%|██████▋   | 14/21 [00:00<00:00, 35.37it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  71%|███████▏  | 15/21 [00:00<00:00, 36.34it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  76%|███████▌  | 16/21 [00:00<00:00, 36.82it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  81%|████████  | 17/21 [00:00<00:00, 37.49it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  86%|████████▌ | 18/21 [00:00<00:00, 38.01it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  90%|█████████ | 19/21 [00:00<00:00, 38.63it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19:  95%|█████████▌| 20/21 [00:00<00:00, 39.22it/s, v_num=95, train_loss=1.660]torch.Size([4, 40, 6])\n",
      "Epoch 19: 100%|██████████| 21/21 [00:00<00:00, 39.51it/s, v_num=95, train_loss=1.660]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 420: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.660]         torch.Size([4, 40, 6])\n",
      "Epoch 20:   5%|▍         | 1/21 [00:00<00:03,  6.51it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  10%|▉         | 2/21 [00:00<00:01, 11.26it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  14%|█▍        | 3/21 [00:00<00:01, 15.22it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  19%|█▉        | 4/21 [00:00<00:00, 18.38it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  24%|██▍       | 5/21 [00:00<00:00, 21.07it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  29%|██▊       | 6/21 [00:00<00:00, 23.31it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  33%|███▎      | 7/21 [00:00<00:00, 25.42it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  38%|███▊      | 8/21 [00:00<00:00, 26.98it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  43%|████▎     | 9/21 [00:00<00:00, 28.40it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  48%|████▊     | 10/21 [00:00<00:00, 29.73it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  52%|█████▏    | 11/21 [00:00<00:00, 31.11it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  57%|█████▋    | 12/21 [00:00<00:00, 32.18it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  62%|██████▏   | 13/21 [00:00<00:00, 32.84it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  67%|██████▋   | 14/21 [00:00<00:00, 33.80it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  71%|███████▏  | 15/21 [00:00<00:00, 34.53it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  76%|███████▌  | 16/21 [00:00<00:00, 35.11it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  81%|████████  | 17/21 [00:00<00:00, 35.95it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  86%|████████▌ | 18/21 [00:00<00:00, 36.69it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  90%|█████████ | 19/21 [00:00<00:00, 37.25it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20:  95%|█████████▌| 20/21 [00:00<00:00, 37.87it/s, v_num=95, train_loss=1.630]torch.Size([4, 40, 6])\n",
      "Epoch 20: 100%|██████████| 21/21 [00:00<00:00, 38.39it/s, v_num=95, train_loss=1.630]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 441: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.630]         torch.Size([4, 40, 6])\n",
      "Epoch 21:   5%|▍         | 1/21 [00:00<00:03,  6.56it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  10%|▉         | 2/21 [00:00<00:01, 11.47it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  14%|█▍        | 3/21 [00:00<00:01, 15.31it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  19%|█▉        | 4/21 [00:00<00:00, 18.61it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  24%|██▍       | 5/21 [00:00<00:00, 21.49it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  29%|██▊       | 6/21 [00:00<00:00, 23.75it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  33%|███▎      | 7/21 [00:00<00:00, 25.80it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  38%|███▊      | 8/21 [00:00<00:00, 27.62it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  43%|████▎     | 9/21 [00:00<00:00, 29.04it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  48%|████▊     | 10/21 [00:00<00:00, 30.62it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  52%|█████▏    | 11/21 [00:00<00:00, 32.10it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  57%|█████▋    | 12/21 [00:00<00:00, 33.32it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  62%|██████▏   | 13/21 [00:00<00:00, 34.30it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  67%|██████▋   | 14/21 [00:00<00:00, 35.50it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  71%|███████▏  | 15/21 [00:00<00:00, 36.01it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  76%|███████▌  | 16/21 [00:00<00:00, 36.88it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  81%|████████  | 17/21 [00:00<00:00, 37.29it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  86%|████████▌ | 18/21 [00:00<00:00, 37.87it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  90%|█████████ | 19/21 [00:00<00:00, 38.41it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21:  95%|█████████▌| 20/21 [00:00<00:00, 38.91it/s, v_num=95, train_loss=1.670]torch.Size([4, 40, 6])\n",
      "Epoch 21: 100%|██████████| 21/21 [00:00<00:00, 39.45it/s, v_num=95, train_loss=1.670]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 462: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.670]         torch.Size([4, 40, 6])\n",
      "Epoch 22:   5%|▍         | 1/21 [00:00<00:03,  6.01it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  10%|▉         | 2/21 [00:00<00:01, 10.50it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  14%|█▍        | 3/21 [00:00<00:01, 14.28it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  19%|█▉        | 4/21 [00:00<00:00, 17.14it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  24%|██▍       | 5/21 [00:00<00:00, 19.74it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  29%|██▊       | 6/21 [00:00<00:00, 21.99it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  33%|███▎      | 7/21 [00:00<00:00, 23.94it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  38%|███▊      | 8/21 [00:00<00:00, 25.74it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  43%|████▎     | 9/21 [00:00<00:00, 27.21it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  48%|████▊     | 10/21 [00:00<00:00, 28.51it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  52%|█████▏    | 11/21 [00:00<00:00, 29.80it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  57%|█████▋    | 12/21 [00:00<00:00, 30.99it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  62%|██████▏   | 13/21 [00:00<00:00, 32.12it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  67%|██████▋   | 14/21 [00:00<00:00, 33.07it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  71%|███████▏  | 15/21 [00:00<00:00, 34.00it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  76%|███████▌  | 16/21 [00:00<00:00, 34.79it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  81%|████████  | 17/21 [00:00<00:00, 35.59it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  86%|████████▌ | 18/21 [00:00<00:00, 36.21it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  90%|█████████ | 19/21 [00:00<00:00, 36.78it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22:  95%|█████████▌| 20/21 [00:00<00:00, 37.40it/s, v_num=95, train_loss=1.420]torch.Size([4, 40, 6])\n",
      "Epoch 22: 100%|██████████| 21/21 [00:00<00:00, 37.87it/s, v_num=95, train_loss=1.420]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 483: 'train_loss' reached 1.31220 (best 1.31220), saving model to '/workspaces/betania.silva/ssl_tools/cpc/lightning_logs/version_95/checkpoints/best_model.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:   0%|          | 0/21 [00:00<?, ?it/s, v_num=95, train_loss=1.420]         torch.Size([4, 40, 6])\n",
      "Epoch 23:   5%|▍         | 1/21 [00:00<00:03,  6.55it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  10%|▉         | 2/21 [00:00<00:01, 11.54it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  14%|█▍        | 3/21 [00:00<00:01, 15.34it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  19%|█▉        | 4/21 [00:00<00:00, 18.69it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  24%|██▍       | 5/21 [00:00<00:00, 21.55it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  29%|██▊       | 6/21 [00:00<00:00, 24.21it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  33%|███▎      | 7/21 [00:00<00:00, 26.15it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  38%|███▊      | 8/21 [00:00<00:00, 27.83it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  43%|████▎     | 9/21 [00:00<00:00, 29.26it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  48%|████▊     | 10/21 [00:00<00:00, 30.54it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  52%|█████▏    | 11/21 [00:00<00:00, 31.86it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  57%|█████▋    | 12/21 [00:00<00:00, 33.21it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  62%|██████▏   | 13/21 [00:00<00:00, 34.47it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  67%|██████▋   | 14/21 [00:00<00:00, 35.66it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  71%|███████▏  | 15/21 [00:00<00:00, 36.72it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n",
      "Epoch 23:  76%|███████▌  | 16/21 [00:00<00:00, 37.71it/s, v_num=95, train_loss=1.310]torch.Size([4, 40, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/betania_meta4/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(cpc, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Diretório onde os checkpoints estão salvos\n",
    "checkpoint_dir = '/workspaces/betania/ssl_tools/cpc/lightning_logs/'\n",
    "\n",
    "# Lista todas as versões no diretório\n",
    "all_versions = [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d)) and d.startswith('version_')]\n",
    "\n",
    "# Ordena as versões em ordem decrescente\n",
    "sorted_versions = sorted(all_versions, key=lambda x: int(x.split('_')[1]), reverse=True)\n",
    "\n",
    "# Obtém a versão mais recente\n",
    "latest_version = sorted_versions[0]\n",
    "\n",
    "VERSION = latest_version\n",
    "\n",
    "# Crie uma instância do modelo\n",
    "\n",
    "cpc = CPC(encoder, density_estimator, auto_regressor)\n",
    "\n",
    "# Carregue os pesos salvos no ponto de verificação 'best_model.ckpt' para o cpc\n",
    "\n",
    "# Mudando a versão do checkpoint\n",
    "\n",
    "checkpoint_path = f'/workspaces/betania/ssl_tools/cpc/lightning_logs/{VERSION}/checkpoints/best_model.ckpt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Carregue os pesos no seu modelo\n",
    "cpc.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CPC(\n",
       "  (encoder): GRUEncoder(\n",
       "    (rnn): GRU(6, 100, bidirectional=True)\n",
       "    (nn): Linear(in_features=200, out_features=10, bias=True)\n",
       "  )\n",
       "  (density_estimator): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (auto_regressor): GRU(10, 10, batch_first=True)\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpc.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPC Fine-Tuning\n",
    "\n",
    "We are going to fine-tune the CPC model on the downstream task of classification. We will use the same dataset and re-use the same encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "# Classificando com uma MLP\n",
    "\n",
    "class StateClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size: int= 10, hidden_size1= 64, hidden_size2=64, num_classes= 7, dropout_prob= 0):\n",
    "        super(StateClassifier, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size2, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "\n",
    " #Linear\n",
    "    \n",
    "# class StateClassifier(torch.nn.Module):\n",
    "#     def __init__(self, input_size: int = 10, n_classes: int = 7):\n",
    "#         super(StateClassifier, self).__init__()\n",
    "#         self.input_size = input_size\n",
    "#         self.n_classes = n_classes\n",
    "#         self.normalize = torch.nn.BatchNorm1d(self.input_size)\n",
    "#         self.nn = torch.nn.Linear(self.input_size, self.n_classes)\n",
    "#         torch.nn.init.xavier_uniform_(self.nn.weight)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.normalize(x)\n",
    "#         logits = self.nn(x)\n",
    "#         return logits\n",
    "\n",
    "class CPC_Classifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: torch.nn.Module,\n",
    "        classifier: torch.nn.Module,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 0.0,\n",
    "        task_class: str = \"multiclass\",\n",
    "        num_classes: int = 7\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.classifier = classifier.to(self.device)\n",
    "        self.learning_rate = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.training_step_losses = []\n",
    "        self.validation_step_losses = []\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        self.task_class = task_class\n",
    "        self.num_classes = num_classes\n",
    "        self.best_validation_loss = float('inf')\n",
    "        \n",
    "    def configure_optimizers(self) -> Any:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.classifier.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encodings = self.encoder(x)\n",
    "        predictions = self.classifier(encodings)\n",
    "        return predictions\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        predictions = self.forward(x)\n",
    "        loss = self.loss_function(predictions, y.long())\n",
    "        self.training_step_losses.append(loss)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.training_step_losses).mean()\n",
    "        self.log(\"train_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        # free up the memory\n",
    "        self.training_step_losses.clear()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        if loss < self.best_validation_loss:\n",
    "            self.best_validation_loss = loss\n",
    "        self.validation_step_losses.append(loss)\n",
    "        metrics = {\"val_acc\": acc, \"val_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "        \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.validation_step_losses).mean()\n",
    "        self.log(\"val_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        if epoch_mean < self.best_validation_loss:\n",
    "            self.best_validation_loss = epoch_mean\n",
    "            self.log(\"best_validation_loss\", self.best_validation_loss, on_epoch=True, on_step=False, prog_bar=False, logger=True)\n",
    "        # free up the memory\n",
    "        self.validation_step_losses.clear()\n",
    "\n",
    "    def _shared_eval_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        predictions = self.forward(x)\n",
    "        loss = self.loss_function(predictions, y.long())\n",
    "        acc = accuracy(torch.argmax(predictions, dim=1), y.long(), task=self.task_class, num_classes=self.num_classes)\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filename='best_model',\n",
    "    monitor='val_loss',  # escolha a métrica que você deseja monitorar (pode ser 'val_acc' ou outra métrica)\n",
    "    mode='min',  # 'min' significa que queremos minimizar a métrica monitorada (por exemplo, perda)\n",
    "    save_top_k=1,  # salva apenas o melhor modelo\n",
    "    save_last=False,  # salva o último modelo\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Caminho dos dados\n",
    "\n",
    "data_path_train = Path(f'/workspaces/betania/data/standartized_balanced/{DATASET_NAME}/train.csv')\n",
    "\n",
    "data_path_validation = Path(f'/workspaces/betania/data/standartized_balanced/{DATASET_NAME}/validation.csv')\n",
    "\n",
    "data_path_test = Path(f'/workspaces/betania/data/standartized_balanced/{DATASET_NAME}/test.csv')\n",
    "\n",
    "# Train\n",
    "\n",
    "x_train = pd.read_csv(data_path_train)\n",
    "\n",
    "x_train = x_train.iloc[:, :360]\n",
    "    \n",
    "x_train = x_train.astype(np.float32)\n",
    "\n",
    "y_train = pd.read_csv(data_path_train)\n",
    "\n",
    "y_train = y_train.iloc[:, -1]\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "tensor_x = torch.Tensor(np.array(x_train))\n",
    "\n",
    "tensor_y = torch.Tensor(np.array(y_train))\n",
    "\n",
    "original_dim = tensor_x.shape[0]\n",
    "\n",
    "input_shape = (original_dim, 6, 60)\n",
    "\n",
    "tensor_x = tensor_x.reshape(input_shape)\n",
    "\n",
    "dataset_train= TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last= True)\n",
    "\n",
    "# Validation\n",
    "\n",
    "x_validation = pd.read_csv(data_path_validation)\n",
    "\n",
    "x_validation = x_validation.iloc[:, :360]\n",
    "\n",
    "x_validation = x_validation.astype(np.float32)\n",
    "\n",
    "y_validation = pd.read_csv(data_path_validation)\n",
    "\n",
    "y_validation = y_validation.iloc[:, -1]\n",
    "\n",
    "y_validation = y_validation.astype(np.float32)\n",
    "\n",
    "tensor_x_val = torch.Tensor(np.array(x_validation))\n",
    "\n",
    "tensor_y_val = torch.Tensor(np.array(y_validation))\n",
    "\n",
    "original_dim_val = tensor_x_val.shape[0]\n",
    "\n",
    "tensor_x_val = tensor_x_val.reshape(original_dim_val, 6, 60)\n",
    "\n",
    "dataset_val= TensorDataset(tensor_x_val,tensor_y_val)\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "\n",
    "# Test\n",
    "\n",
    "x_test = pd.read_csv(data_path_test)\n",
    "\n",
    "x_test = x_test.iloc[:, :360]\n",
    "\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "y_test = pd.read_csv(data_path_test)\n",
    "\n",
    "y_test = y_test.iloc[:, -1]\n",
    "\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "tensor_x_test = torch.Tensor(np.array(x_test))\n",
    "\n",
    "tensor_y_test = torch.Tensor(np.array(y_test))\n",
    "\n",
    "original_dim_test = tensor_x_test.shape[0]\n",
    "\n",
    "tensor_x_test = tensor_x_test.reshape((original_dim_test, 6, 60))\n",
    "\n",
    "dataset_test= TensorDataset(tensor_x_test,tensor_y_test)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True, drop_last= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_size = 10\n",
    "n_classes = 7\n",
    "\n",
    "\n",
    "#classifier = StateClassifier(input_size=encoding_size, n_classes=n_classes)\n",
    "\n",
    "classifier = StateClassifier(input_size=encoding_size, num_classes=n_classes, dropout_prob=0, hidden_size1= 64, hidden_size2=32)\n",
    "cpc_classifier = CPC_Classifier(encoder, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpc_classifier = cpc_classifier.to('cuda')\n",
    "\n",
    "# Forward no conjunto de teste\n",
    "with torch.no_grad():\n",
    "    xtest = cpc_classifier.forward(tensor_x_test.to('cuda')).cpu().numpy()\n",
    "\n",
    "# Forward no conjunto de treinamento\n",
    "with torch.no_grad():\n",
    "    xtrain = cpc_classifier.forward(tensor_x.to('cuda')).cpu().numpy()\n",
    "\n",
    "# Converter rótulos para NumPy\n",
    "ytrain = tensor_y.cpu().numpy()\n",
    "ytest = tensor_y_test.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A precisão do modelo no conjunto de teste é: 0.5579710144927537\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "random_forest_model.fit(xtrain, ytrain)\n",
    "\n",
    "# Faça previsões no conjunto de teste\n",
    "predictions = random_forest_model.predict(xtest)\n",
    "\n",
    "# Avalie o desempenho do modelo usando métricas, como a precisão\n",
    "accuracy = accuracy_score(ytest, predictions)\n",
    "\n",
    "print(f'A precisão do modelo no conjunto de teste é: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (KNN): 0.5420289855072464\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Criar o modelo KNN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=20)  # Pode ajustar o número de vizinhos conforme necessário\n",
    "\n",
    "# Treinar o modelo\n",
    "knn_model.fit(xtrain, ytrain)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "predictions_knn = knn_model.predict(xtest)\n",
    "\n",
    "# Avaliar o desempenho do modelo usando métricas, como a precisão\n",
    "accuracy_knn = accuracy_score(ytest, predictions_knn)\n",
    "\n",
    "print(\"Accuracy (KNN):\", accuracy_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (SVM): 0.3101449275362319\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Criar o modelo SVM\n",
    "svm_model = SVC(kernel='linear', C=1.0)  # Pode ajustar o kernel e C conforme necessário\n",
    "\n",
    "# Treinar o modelo\n",
    "svm_model.fit(xtrain, ytrain)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "predictions_svm = svm_model.predict(xtest)\n",
    "\n",
    "# Avaliar o desempenho do modelo usando métricas, como a precisão\n",
    "accuracy_svm = accuracy_score(ytest, predictions_svm)\n",
    "\n",
    "print(\"Accuracy (SVM):\", accuracy_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer = pl.Trainer(max_epochs=300, accelerator=\"gpu\", devices=1, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.fit(cpc_classifier, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CPC_Classifier:\n\tMissing key(s) in state_dict: \"classifier.layer1.0.weight\", \"classifier.layer1.0.bias\", \"classifier.layer2.0.weight\", \"classifier.layer2.0.bias\", \"classifier.output_layer.0.weight\", \"classifier.output_layer.0.bias\". \n\tUnexpected key(s) in state_dict: \"density_estimator.weight\", \"density_estimator.bias\", \"auto_regressor.weight_ih_l0\", \"auto_regressor.weight_hh_l0\", \"auto_regressor.bias_ih_l0\", \"auto_regressor.bias_hh_l0\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(checkpoint_path, map_location\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[39m# Carregue os pesos no seu classificador\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m cpc_classifier\u001b[39m.\u001b[39;49mload_state_dict(checkpoint[\u001b[39m'\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2056\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2051\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2052\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2053\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2056\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2057\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2058\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CPC_Classifier:\n\tMissing key(s) in state_dict: \"classifier.layer1.0.weight\", \"classifier.layer1.0.bias\", \"classifier.layer2.0.weight\", \"classifier.layer2.0.bias\", \"classifier.output_layer.0.weight\", \"classifier.output_layer.0.bias\". \n\tUnexpected key(s) in state_dict: \"density_estimator.weight\", \"density_estimator.bias\", \"auto_regressor.weight_ih_l0\", \"auto_regressor.weight_hh_l0\", \"auto_regressor.bias_ih_l0\", \"auto_regressor.bias_hh_l0\". "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Diretório onde os checkpoints estão salvos\n",
    "checkpoint_dir = '/workspaces/betania/ssl_tools/cpc/lightning_logs/'\n",
    "\n",
    "# Lista todas as versões no diretório\n",
    "all_versions = [d for d in os.listdir(checkpoint_dir) if os.path.isdir(os.path.join(checkpoint_dir, d)) and d.startswith('version_')]\n",
    "\n",
    "# Ordena as versões em ordem decrescente\n",
    "sorted_versions = sorted(all_versions, key=lambda x: int(x.split('_')[1]), reverse=True)\n",
    "\n",
    "# Obtém a versão mais recente\n",
    "latest_version = sorted_versions[0]\n",
    "\n",
    "VERSION = latest_version\n",
    "\n",
    "# Carregue os pesos salvos no ponto de verificação 'best_model.ckpt' para o cpc\n",
    "\n",
    "# Mudando a versão do checkpoint\n",
    "\n",
    "checkpoint_path = f'/workspaces/betania/ssl_tools/cpc/lightning_logs/{VERSION}/checkpoints/best_model.ckpt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Carregue os pesos no seu classificador\n",
    "cpc_classifier.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 6/6 [00:00<00:00, 75.09it/s] \n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.621666669845581\n",
      "        test_loss            1.544309377670288\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.621666669845581, 'test_loss': 1.544309377670288}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(cpc_classifier, dataloader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
