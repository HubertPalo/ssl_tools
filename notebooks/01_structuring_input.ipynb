{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Structuring the input\n",
    "\n",
    "In order to train and test models, we need to structure the input data in a way that is compatible with the model and framweork's experiments.\n",
    "\n",
    "This framework is designed to work with time-series data and Pytorch Lightning. \n",
    "Thus, it provides the necessary tools to create a `Dataset` object and a `LightningDataModule` object.\n",
    "\n",
    "The `Dataset` object is responsible for loading the data. It is a Pytorch object that is used to load the data and make it available to the model. \n",
    "Every `Dataset` class must implement two methods: `__len__` and `__getitem__`.\n",
    "The `__len__` method returns the number of samples in the dataset, and the `__getitem__`, given an integer from 0 to `__len__` - 1, returns the corresponding sample from the dataset.\n",
    "The returned type of the `__getitem__` method is not specified, but it is usually a 2-element tuple with the input and the target. The input is the data that will be used to make the predictions, and the target is the data that the model will try to predict.\n",
    "\n",
    "For now, this framework provide implementations for the `Dataset` objects for time-series data, where data is organized in two different ways:\n",
    "- A directory with several CSV files, where each file contains a time-series. Each row in a CSV file is a time-step, each column is a feature, and the whole file is a time-series. This is handled by the `SeriesFolderCSVDataset` class.\n",
    "- A single CSV file with a windowed time-series. Each row in the CSV file is a window, and each column is a feature. This is handled by the `MultiModalSeriesCSVDataset` class.\n",
    "\n",
    "We explain both classes in detais nextly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-series dataset implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SeriesFolderCSVDataset\n",
    "\n",
    "The `SeriesFolderCSVDataset` class is designed to work with a directory containing several CSV files, where each file represent a time-series. \n",
    "Each row in a CSV file is a time-step, and each column is a feature. \n",
    "\n",
    "This class assumes that data is organized in the following way, where `my_dataset` is the path to the directory containing the CSV files:\n",
    "\n",
    "```bash\n",
    "my_dataset/\n",
    "    series1.csv\n",
    "    series2.csv\n",
    "    other_series.csv\n",
    "    ...\n",
    "```\n",
    "\n",
    "Where each CSV file represents a time-series. \n",
    "\n",
    "| accel-x | accel-y | accel-z | gyro-x  | gyro-y  | gyro-z  | class   |\n",
    "|---------|---------|---------|---------|---------|---------|---------|\n",
    "| 0.502123| 0.02123 | 0.12312 | 0.12312 | 0.12312 | 0.12312 | 1       |\n",
    "| 0.682012| 0.02123 | 0.12312 | 0.12312 | 0.12312 | 0.12312 | 1       |\n",
    "| 0.498217| 0.00001 | 0.12312 | 0.12312 | 0.12312 | 0.12312 | 1       |\n",
    "\n",
    "\n",
    "Note that the CSV must have a header with the column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle this kind of data, we use the `SeriesFolderCSVDataset` class. This class is a Pytorch `Dataset` object that loads the data from the CSV files and makes it available to the model.\n",
    "For this class, we must specify the path to the directory containing the CSV files, the name of the columns that will be used as features, and the name of the column that will be used as the target.\n",
    "Note that, each feature (column) represent a dimension of the time-series, while the rows represent the time-steps.\n",
    "\n",
    "Thus, the `SeriesFolderCSVDataset` class minimally requires:\n",
    "- `data_path`: the path to the directory containing the CSV files\n",
    "- `features`: a list of strings with the names of the features columns, e.g. `['accel-x', 'accel-y', 'accel-z', 'gyro-x', 'gyro-y', 'gyro-z']`\n",
    "- `label`: a string with the name of the label column, e.g. `'class'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SeriesFolderCSVDataset at /workspaces/hiaac-m4/ssl_tools/data/view_concatenated/KuHar_cpc/train (57 samples)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ssl_tools.data.datasets import SeriesFolderCSVDataset\n",
    "\n",
    "# Path to the data\n",
    "data_path = \"/workspaces/hiaac-m4/ssl_tools/data/view_concatenated/KuHar_cpc/train\"\n",
    "\n",
    "# Creating the dataset\n",
    "dataset = SeriesFolderCSVDataset(\n",
    "    data_path=data_path,\n",
    "    features=[\"accel-x\", \"accel-y\", \"accel-z\", \"gyro-x\", \"gyro-y\", \"gyro-z\"],\n",
    "    label=\"standard activity code\",\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the number of samples in the dataset with the `len` function, and we can retrive a sample with the `__getitem__` method, that is, using `[]`, such as `dataset[0]`.\n",
    "The dataset may return:\n",
    "- A 2-element tuple, where the first element is a 2D numpy array with shape `(num_features, time_steps)`, and the second element is a 1D tensor with shape `(time_steps,)`.\n",
    "- A 2D numpy array with shape `(num_features, time_steps)`, if `label` is `None`, at the time of the dataset object's creation.\n",
    "\n",
    "Let's check the number of samples and access the first sample and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 57 samples\n"
     ]
    }
   ],
   "source": [
    "# Gte the length of the dataset\n",
    "length_of_dataset = len(dataset)\n",
    "print(f\"Length of dataset: {length_of_dataset} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of sample: tuple with 2 elements\n"
     ]
    }
   ],
   "source": [
    "# Get the first sample\n",
    "sample = dataset[0]\n",
    "type_of_sample = type(sample).__name__\n",
    "print(f\"Type of sample: {type_of_sample} with {len(sample)} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sample: (6, 2586), shape of label: (2586, 1)\n"
     ]
    }
   ],
   "source": [
    "# The first element of the sample is the input, while the second element is the label\n",
    "# We can split the sample into input and label variables\n",
    "shape_of_sample = sample[0].shape\n",
    "shape_of_label = sample[1].shape\n",
    "print(f\"Shape of sample: {shape_of_sample}, shape of label: {shape_of_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above, the sample is a 2-element tuple. The first element is a 2D numpy array with shape `(6, 2586)`, and the second element is a 1D tensor with shape `(2586,)`, that is, a label for each time-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiModalSeriesCSVDataset\n",
    "\n",
    "\n",
    "The `MultiModalSeriesCSVDataset` class is designed to work with a single CSV file containing a windowed time-series. \n",
    "The CSV is a multi-modal time-series, where each row is a sample and each column is a feature at a given time-step. \n",
    "Features are organized in a way that each group of columns represent a different modality.\n",
    "\n",
    "The CSV file looks like this:\n",
    "\n",
    "\n",
    "| accel-x-0 | accel-x-1 | accel-y-0 | accel-y-1 |  class |\n",
    "|-----------|-----------|-----------|-----------|--------|\n",
    "| 0.502123  | 0.02123   | 0.502123  | 0.502123  |  0     |\n",
    "| 0.6820123 | 0.02123   | 0.502123  | 0.502123  |  1     |\n",
    "| 0.498217  | 0.00001   | 1.414141  | 3.141592  |  2     |\n",
    "\n",
    "In the example, columns `accel-x-0` and `accel-x-1` are the `accel-x` feature at time `0` and time `1`, respectively. The same goes for the `accel-y` feature. Finally, the `class` column is the label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle this kind of data, we use the `MultiModalSeriesCSVDataset` class.\n",
    "For this class, we must specify the path to the CSV file, the prefix of the columns that will be used as features, and the columns that will be used as label.\n",
    "Note that, each feature (column) represent a dimension of the time-series, while the rows represent the samples.\n",
    "\n",
    "The `MultiModalSeriesCSVDataset` class minimally requires:\n",
    "- `data_path`: the path to the CSV file\n",
    "- `feature_prefixes`: a list of strings with the prefixes of the feature columns, e.g. `['accel-x', 'accel-y']`. The class will look for columns with these prefixes and will consider them as features of a modality.\n",
    "- `label`: a string with the name of the label column, e.g. `'class'`\n",
    "- `features_as_channels`: a boolean indicating if the features should be treated as channels, that is, if each prefix will become a channel. If ``True``, the data will be returned as a vector of shape `(C, T)`, where C is the number of channels (features/prefixes) and `T` is the number of time steps. Else, the data will be returned as a vector of shape `T*C` (a single vector with all the features).\n",
    "\n",
    "Let's show how to read this data and create a `MultiModalSeriesCSVDataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalSeriesCSVDataset at /workspaces/hiaac-m4/ssl_tools/data/standartized_balanced/KuHar/train.csv (1386 samples)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ssl_tools.data.datasets import MultiModalSeriesCSVDataset\n",
    "\n",
    "# Path to the data\n",
    "data_path = \"/workspaces/hiaac-m4/ssl_tools/data/standartized_balanced/KuHar/train.csv\"\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = MultiModalSeriesCSVDataset(\n",
    "    data_path=data_path,\n",
    "    feature_prefixes=[\"accel-x\", \"accel-y\", \"accel-z\", \"gyro-x\", \"gyro-y\", \"gyro-z\"],\n",
    "    label=\"standard activity code\",\n",
    "    features_as_channels = True,\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the number of samples in the dataset with the `len` function, and we can retrive a sample with the `__getitem__` method, that is, using `[]`, such as `dataset[0]`.\n",
    "The dataset may return:\n",
    "- A 2-element tuple, where the first element is a 2D numpy array with shape `(num_features, time_steps)`, and the second element is a 1D tensor with shape `(time_steps,)`.\n",
    "- A 2D numpy array with shape `(num_features, time_steps)`, if `label` is `None`, at the time of the dataset object's creation.\n",
    "\n",
    "Let's check the number of samples and access the first sample and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 1386 samples\n"
     ]
    }
   ],
   "source": [
    "length_of_dataset = len(dataset)\n",
    "print(f\"Length of dataset: {length_of_dataset} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of sample: tuple with length 2 elements\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "type_of_sample = type(sample).__name__\n",
    "print(f\"Type of sample: {type_of_sample} with length {len(sample)} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sample: (6, 60)\n"
     ]
    }
   ],
   "source": [
    "shape_of_sample = sample[0].shape\n",
    "print(f\"Shape of sample: {shape_of_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading batches of data\n",
    "\n",
    "Pytorch models are trained using batches of data. Thus, we do not feed the model with a single sample at a time, but with a batch of samples.\n",
    "If we see the last example, the `MultiModalSeriesCSVDataset` object returns a single sample at a time. Each sample is a 2-element tuple, where first element is a `(6, 60)` numpy array and the second is an integer, representing the label.\n",
    "\n",
    "A batch of samples add an extra dimension to the data. Thus, in our case, a batch of samples is a 3D tensor, where the first dimension is the batch size (`B`), the second dimension is the number of features, or channels (`C`), and the third dimension is the number of time-steps (`T`).\n",
    "Thus, if the data have the shape `(6, 60)`, a batch of samples will have the shape `(B, 6, 60)`. The same happens to `label`, which gains an extra dimension. \n",
    "\n",
    "The batching of samples is done using a `DataLoader` object. This object is a Pytorch object that takes a `Dataset` object and returns batches of samples. The `DataLoader` object is responsible for shuffling the data, dividing it into batches, and loading the data in parallel.\n",
    "Thus, given a `Dataset` object, we can easilly create a `DataLoader` object using the `torch.utils.data.DataLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f786700c580>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fetch a batch of samples from the `DataLoader` object using the `__iter__` method, that is, using a `for` loop. Each iteration returns a batch of samples.\n",
    "In our case, each batch is a 2-element tuple, where the first element is a 3D tensor with shape `(B, C, T)`, and the second element is a 1D tensor with shape `(B,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([32, 6, 60]), labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling data splits (train, validation, and test)\n",
    "\n",
    "Usually, we create a `DataLoader` object for the training data, another for the validation data, and another for the test data. \n",
    "A simple way to do this is to create a `LightningDataModule` object, which is a Pytorch Lightning object that is responsible for creating the `DataLoader` objects for the training, validation, and test data.\n",
    "\n",
    "A `LightningDataModule` object is responsible for splitting the data into training, validation, and test sets, and creating the `DataLoader` objects for each set. This object may also be responsible for setting up the data, such as downloading the data from the internet, checking the data, and add the augmentations. This module is used to encapsulate all the data loading and processing logic in a single place, and to make it easy to use the same data processing logic across different experiments.\n",
    "\n",
    "The `LightningDataModule` object must implement three methods: `setup`, `train_dataloader`, and `val_dataloader`. The `setup` is optional, and is responsible for splitting the data into training, validation, and test sets, and the `train_dataloader` and `val_dataloader` methods are responsible for creating the `DataLoader` objects for the training and validation sets, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "class MyDataModule(L.LightningDataModule):\n",
    "    def __init__(self, train_csv_path, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.train_csv_path = train_csv_path\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = MultiModalSeriesCSVDataset(\n",
    "                data_path=self.train_csv_path,\n",
    "                feature_prefixes=[\"accel-x\", \"accel-y\", \"accel-z\", \"gyro-x\", \"gyro-y\", \"gyro-z\"],\n",
    "                label=\"standard activity code\",\n",
    "                features_as_channels=True,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a Pytorch Lightning model, we pass a `LightningDataModule` object to the `Trainer` object, and the `Trainer` object is responsible calling the `setup`, `train_dataloader`, and `val_dataloader` methods, and for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "First, we need to check which is our dataset and how the data is organized. If the data is organized in a directory with several CSV files, we use the `SeriesFolderCSVDataset` class. If the data is organized in a single CSV file with a windowed time-series, we use the `MultiModalSeriesCSVDataset` class.\n",
    "\n",
    "Then, we create a `Dataset` object, and use it to create a `DataLoader` object.\n",
    "\n",
    "Instead of creating a `Dataset` and a `DataLoader` object for the training, validation, and test data, we can use a `LightningDataModule` object to create them all at once, on demand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
