{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando bibliotecas\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'UCI'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPC SSL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de Treino\n",
    "\n",
    "data_path = Path('/workspaces/betania/ssl_tools/view_concatenated/UCI_cpc/train')\n",
    "\n",
    "datas_x_train = []\n",
    "\n",
    "data_y_train = []\n",
    "\n",
    "for f in data_path.glob('*.csv'):\n",
    "    data = pd.read_csv(f)\n",
    "    x = data[['accel-x', 'accel-y', 'accel-z', 'gyro-x', 'gyro-y', 'gyro-z']].values\n",
    "\n",
    "    # Expend dimension\n",
    "\n",
    "    x = np.swapaxes(x, 1, 0)\n",
    "    \n",
    "    datas_x_train.append(x)\n",
    "\n",
    "    y = data['standard activity code'].values\n",
    "\n",
    "    data_y_train.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a função shape para obter o shape de cada array na lista\n",
    "formas = [arr.shape for arr in datas_x_train]\n",
    "\n",
    "# Isso irá imprimir as formas (shapes) de cada array na lista\n",
    "for i, forma in enumerate(formas):\n",
    "    print(f\"Array {i+1}: Shape {forma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de Teste\n",
    "\n",
    "data_path = Path('/workspaces/betania/ssl_tools/view_concatenated/UCI_cpc/test')\n",
    "\n",
    "datas_x_test = []\n",
    "\n",
    "data_y_test = []\n",
    "\n",
    "for f in data_path.glob('*.csv'):\n",
    "    data = pd.read_csv(f)\n",
    "    x = data[['accel-x', 'accel-y', 'accel-z', 'gyro-x', 'gyro-y', 'gyro-z']].values\n",
    "\n",
    "    # Expend dimension\n",
    "\n",
    "    x = np.swapaxes(x, 1, 0)\n",
    "    \n",
    "    datas_x_test.append(x)\n",
    "\n",
    "    y = data['standard activity code'].values\n",
    "\n",
    "    data_y_test.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = datas_x_train\n",
    "y_train = data_y_train\n",
    "x_test = datas_x_test\n",
    "y_test = data_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, num_channels, kernel_size=3, dropout_rate=0.2, device: str = \"cpu\"):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(num_channels, 32, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)  # Mova os dados para a GPU, se disponível\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GRUEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel: int = 128,\n",
    "        encoding_size: int = 256,\n",
    "        bidirectional: bool = True,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.encoding_size = encoding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.device = device\n",
    "\n",
    "        # Defina uma camada GRU com duas camadas e 256 unidades em cada camada\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=in_channel,\n",
    "            hidden_size=256,  # 256 unidades em cada camada\n",
    "            num_layers=2,      # Duas camadas GRU\n",
    "            batch_first=False,\n",
    "            bidirectional=bidirectional,\n",
    "        ).to(device)\n",
    "\n",
    "        self.nn = nn.Linear(\n",
    "            256 * (int(bidirectional) + 1), self.encoding_size\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)\n",
    "\n",
    "        past = torch.zeros(\n",
    "            4,  # Duas camadas com bidirecionalidade resulta em 4 direções\n",
    "            x.shape[1],\n",
    "            256,  # Tamanho das unidades\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        out, _ = self.gru(x, past)\n",
    "        encodings = self.nn(out[-1].squeeze(0))\n",
    "        return encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoderCNN: torch.nn.Module,\n",
    "        encoder: torch.nn.Module,\n",
    "        density_estimator: torch.nn.Module,\n",
    "        auto_regressor: torch.nn.Module,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 0.0,\n",
    "        window_size: int = 4,\n",
    "        n_size: int = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoderCNN = encoderCNN.to(self.device)\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.density_estimator = density_estimator.to(self.device)\n",
    "        self.auto_regressor = auto_regressor.to(self.device)\n",
    "        self.learning_rate = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.window_size = window_size\n",
    "        self.n_size = n_size\n",
    "        self.training_step_losses = []\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        assert len(batch) == 1, \"Batch must be 1 sample only\"\n",
    "        sample = batch\n",
    "        sample = sample.squeeze(0)\n",
    "        rnd_t = np.random.randint(\n",
    "            5 * self.window_size, sample.shape[-1] - 5 * self.window_size\n",
    "        )\n",
    "        sample = torch.tensor(\n",
    "            sample[\n",
    "                :,\n",
    "                max(0, (rnd_t - 20 * self.window_size)) : min(\n",
    "                    sample.shape[-1], rnd_t + 20 * self.window_size\n",
    "                ),\n",
    "            ]\n",
    "        ).cpu()\n",
    "\n",
    "        T = sample.shape[-1]\n",
    "        windowed_sample = np.split(\n",
    "            sample[:, : (T // self.window_size) * self.window_size],\n",
    "            (T // self.window_size),\n",
    "            -1,\n",
    "        )\n",
    "        windowed_sample = torch.tensor(np.stack(windowed_sample, 0), device=self.device)\n",
    "\n",
    "        # Obtém as saídas do codificador CNN\n",
    "        encodings_cnn = self.encoderCNN(windowed_sample)\n",
    "\n",
    "        encodings = self.encoder(encodings_cnn)\n",
    "\n",
    "        window_ind = torch.randint(2, len(encodings) - 2, size=(1,))\n",
    "        _, c_t = self.auto_regressor(\n",
    "            encodings[max(0, window_ind[0] - 10) : window_ind[0] + 1].unsqueeze(0)\n",
    "        )\n",
    "        density_ratios = torch.bmm(\n",
    "            encodings.unsqueeze(1),\n",
    "            self.density_estimator(c_t.squeeze(1).squeeze(0)).expand_as(encodings).unsqueeze(-1),\n",
    "        ).view(\n",
    "            -1,\n",
    "        )\n",
    "        r = set(range(0, window_ind[0] - 2))\n",
    "        r.update(set(range(window_ind[0] + 3, len(encodings))))\n",
    "        rnd_n = np.random.choice(list(r), self.n_size)\n",
    "        X_N = torch.cat(\n",
    "            [density_ratios[rnd_n], density_ratios[window_ind[0] + 1].unsqueeze(0)], 0\n",
    "        ).to(self.device)\n",
    "        labels = torch.Tensor([len(X_N) - 1]).to(self.device)\n",
    "        loss = torch.nn.CrossEntropyLoss()(X_N.view(1, -1), labels.long())\n",
    "        self.training_step_losses.append(loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.training_step_losses).mean()\n",
    "        self.log(\"train_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        # free up the memory\n",
    "        self.training_step_losses.clear()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        learnable_parameters = (\n",
    "            list(self.density_estimator.parameters())\n",
    "            + list(self.encoder.parameters())\n",
    "            + list(self.auto_regressor.parameters())\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            learnable_parameters, lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 6\n",
    "\n",
    "encoding_size=256\n",
    "\n",
    "# Cria o codificador\n",
    "encoderCNN = CNNEncoder(num_channels, device='cuda')\n",
    "\n",
    "# Cria o modelo autoregressivo com GRU\n",
    "encoder = GRUEncoder(in_channel=128, encoding_size=256, device='cuda')\n",
    "\n",
    "density_estimator = torch.nn.Linear(encoding_size, encoding_size)\n",
    "\n",
    "auto_regressor = torch.nn.GRU(\n",
    "    input_size=encoding_size, \n",
    "    hidden_size=encoding_size, \n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "cpc = CPC(encoderCNN, encoder, density_estimator, auto_regressor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleDataset:\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx].astype(np.float32), self.y[idx].astype(np.float32)\n",
    "        else:\n",
    "            return self.X[idx].astype(np.float32)\n",
    "    \n",
    "train_dataset = SimpleDataset(x_train)\n",
    "test_dataset = SimpleDataset(x_test, y_test)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=10, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10, accelerator=\"gpu\", devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name              | Type       | Params\n",
      "-------------------------------------------------\n",
      "0 | encoderCNN        | CNNEncoder | 31.5 K\n",
      "1 | encoder           | GRUEncoder | 189 K \n",
      "2 | density_estimator | Linear     | 65.8 K\n",
      "3 | auto_regressor    | GRU        | 394 K \n",
      "-------------------------------------------------\n",
      "681 K     Trainable params\n",
      "0         Non-trainable params\n",
      "681 K     Total params\n",
      "2.726     Total estimated model params size (MB)\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:104: Total length of `CombinedLoader` across ranks is zero. Please make sure this was your intention.\n",
      "`Trainer.fit` stopped: No training batches.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(cpc, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking if there are weights saved from previous training, else it trains from scratch\n",
    "# try:\n",
    "#     encoder.load_state_dict(torch.load(f'./weights/{DATASET_NAME}_encoder.pt'))\n",
    "# except:\n",
    "#     torch.save(encoder.state_dict(), f'./weights/{DATASET_NAME}_encoder.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPC Fine-Tuning\n",
    "\n",
    "We are going to fine-tune the CPC model on the downstream task of classification. We will use the same dataset and re-use the same encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Any, Optional\n",
    "\n",
    "# from torchmetrics.functional import accuracy\n",
    "\n",
    "# class StateClassifier(torch.nn.Module):\n",
    "#     def __init__(self, input_size: int = 10, n_classes: int = 7):\n",
    "#         super(StateClassifier, self).__init__()\n",
    "#         self.input_size = input_size\n",
    "#         self.n_classes = 7\n",
    "#         self.normalize = torch.nn.BatchNorm1d(self.input_size)\n",
    "#         self.nn = torch.nn.Linear(self.input_size, self.n_classes)\n",
    "#         torch.nn.init.xavier_uniform_(self.nn.weight)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.normalize(x)\n",
    "#         logits = self.nn(x)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# class CPC_Classifier(pl.LightningModule):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         encoderCNN: torch.nn.Module,\n",
    "#         encoder: torch.nn.Module,\n",
    "#         classifier: torch.nn.Module,\n",
    "#         lr: float = 1e-3,\n",
    "#         weight_decay: float = 0.0,\n",
    "#         task_class: str = \"multiclass\",\n",
    "#         num_classes: int = 7\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.encoderCNN = encoderCNN.to('cpu')\n",
    "#         self.encoder = encoder.to('cpu')\n",
    "#         self.classifier = classifier.to('cpu')\n",
    "#         self.learning_rate = lr\n",
    "#         self.weight_decay = weight_decay\n",
    "#         self.training_step_losses = []\n",
    "#         self.validation_step_losses = []\n",
    "#         self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "#         self.task_class = task_class\n",
    "#         self.num_classes = 7\n",
    "        \n",
    "#     def configure_optimizers(self) -> Any:\n",
    "#         optimizer = torch.optim.Adam(\n",
    "#             self.classifier.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "#         )\n",
    "#         return optimizer\n",
    "    \n",
    "#         # def configure_optimizers(self):\n",
    "#         # learnable_parameters = (\n",
    "#         #     list(self.encoderCNN.parameters())\n",
    "#         #     + list(self.encoder.parameters())\n",
    "#         #     + list(self.classifier.parameters())\n",
    "#         # )\n",
    "\n",
    "#         # optimizer = torch.optim.Adam(\n",
    "#         #     learnable_parameters, lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "#         # )\n",
    "#         # return optimizer\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         encodings_cnn = self.encoderCNN(x)\n",
    "#         encodings = self.encoder(encodings_cnn)\n",
    "#         #predictions = self.classifier(encodings)\n",
    "#         #return predictions\n",
    "#         return encodings\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         predictions = self.forward(x)\n",
    "#         loss = self.loss_function(predictions, y.long())\n",
    "#         self.training_step_losses.append(loss)\n",
    "#         return loss\n",
    "    \n",
    "#     def on_train_epoch_end(self) -> None:\n",
    "#         # do something with all training_step outputs, for example:\n",
    "#         epoch_mean = torch.stack(self.training_step_losses).mean()\n",
    "#         self.log(\"train_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "#         # free up the memory\n",
    "#         self.training_step_losses.clear()\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "#         self.validation_step_losses.append(loss)\n",
    "#         metrics = {\"val_acc\": acc, \"val_loss\": loss}\n",
    "#         self.log_dict(metrics)\n",
    "#         return metrics\n",
    "    \n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "#         metrics = {\"test_acc\": acc, \"test_loss\": loss}\n",
    "#         self.log_dict(metrics)\n",
    "#         return metrics\n",
    "\n",
    "        \n",
    "#     def on_validation_epoch_end(self) -> None:\n",
    "#         # do something with all training_step outputs, for example:\n",
    "#         epoch_mean = torch.stack(self.validation_step_losses).mean()\n",
    "#         self.log(\"val_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "#         # free up the memory\n",
    "#         self.validation_step_losses.clear()\n",
    "\n",
    "#     def _shared_eval_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         predictions = self.forward(x)\n",
    "#         loss = self.loss_function(predictions, y.long())\n",
    "#         acc = accuracy(torch.argmax(predictions, dim=1), y.long(), task=self.task_class, num_classes=self.num_classes)\n",
    "#         return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "# Classificando com uma MLP\n",
    "\n",
    "class StateClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size: int= 10, hidden_size1= 64, hidden_size2=64, n_classes= 7, dropout_prob= 0):\n",
    "        super(StateClassifier, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size2, n_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "\n",
    "class CPC_Classifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoderCNN: torch.nn.Module,\n",
    "        encoder: torch.nn.Module,\n",
    "        classifier: torch.nn.Module,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 0.0,\n",
    "        task_class: str = \"multiclass\",\n",
    "        num_classes: int = 7\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoderCNN = encoderCNN.to(self.device)\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.classifier = classifier.to(self.device)\n",
    "        self.learning_rate = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.training_step_losses = []\n",
    "        self.validation_step_losses = []\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        self.task_class = task_class\n",
    "        self.num_classes = 7\n",
    "        \n",
    "    def configure_optimizers(self) -> Any:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.classifier.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encodings_cnn = self.encoderCNN(x)\n",
    "        encodings = self.encoder(encodings_cnn)\n",
    "        predictions = self.classifier(encodings)\n",
    "        return predictions\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        predictions = self.forward(x)\n",
    "        loss = self.loss_function(predictions, y.long())\n",
    "        self.training_step_losses.append(loss)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.training_step_losses).mean()\n",
    "        self.log(\"train_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        # free up the memory\n",
    "        self.training_step_losses.clear()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        self.validation_step_losses.append(loss)\n",
    "        metrics = {\"val_acc\": acc, \"val_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "        \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.validation_step_losses).mean()\n",
    "        self.log(\"val_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        # free up the memory\n",
    "        self.validation_step_losses.clear()\n",
    "\n",
    "    def _shared_eval_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        predictions = self.forward(x)\n",
    "        loss = self.loss_function(predictions, y.long())\n",
    "        acc = accuracy(torch.argmax(predictions, dim=1), y.long(), task=self.task_class, num_classes=self.num_classes)\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Caminho dos dados\n",
    "\n",
    "data_path_train = Path('/workspaces/betania/data/standartized_balanced/UCI/train.csv')\n",
    "\n",
    "data_path_validation = Path('/workspaces/betania/data/standartized_balanced/UCI/validation.csv')\n",
    "\n",
    "data_path_test = Path('/workspaces/betania/data/standartized_balanced/UCI/test.csv')\n",
    "\n",
    "# Train\n",
    "\n",
    "x_train = pd.read_csv(data_path_train)\n",
    "\n",
    "x_train = x_train.iloc[:, :360]\n",
    "    \n",
    "x_train = x_train.astype(np.float32)\n",
    "\n",
    "y_train = pd.read_csv(data_path_train)\n",
    "\n",
    "y_train = y_train.iloc[:, -1]\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "tensor_x = torch.Tensor(np.array(x_train))\n",
    "\n",
    "tensor_y = torch.Tensor(np.array(y_train))\n",
    "\n",
    "original_dim = tensor_x.shape[0]\n",
    "\n",
    "input_shape = (original_dim, 6, 60)\n",
    "\n",
    "tensor_x = tensor_x.reshape(input_shape)\n",
    "\n",
    "dataset_train= TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last= True)\n",
    "\n",
    "# Validation\n",
    "\n",
    "x_validation = pd.read_csv(data_path_validation)\n",
    "\n",
    "x_validation = x_validation.iloc[:, :360]\n",
    "\n",
    "x_validation = x_validation.astype(np.float32)\n",
    "\n",
    "y_validation = pd.read_csv(data_path_validation)\n",
    "\n",
    "y_validation = y_validation.iloc[:, -1]\n",
    "\n",
    "y_validation = y_validation.astype(np.float32)\n",
    "\n",
    "tensor_x_val = torch.Tensor(np.array(x_validation))\n",
    "\n",
    "tensor_y_val = torch.Tensor(np.array(y_validation))\n",
    "\n",
    "original_dim_val = tensor_x_val.shape[0]\n",
    "\n",
    "tensor_x_val = tensor_x_val.reshape(original_dim_val, 6, 60)\n",
    "\n",
    "dataset_val= TensorDataset(tensor_x_val,tensor_y_val)\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "\n",
    "# Test\n",
    "\n",
    "x_test = pd.read_csv(data_path_test)\n",
    "\n",
    "x_test = x_test.iloc[:, :360]\n",
    "\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "y_test = pd.read_csv(data_path_test)\n",
    "\n",
    "y_test = y_test.iloc[:, -1]\n",
    "\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "tensor_x_test = torch.Tensor(np.array(x_test))\n",
    "\n",
    "tensor_y_test = torch.Tensor(np.array(y_test))\n",
    "\n",
    "original_dim_test = tensor_x_test.shape[0]\n",
    "\n",
    "tensor_x_test = tensor_x_test.reshape((original_dim_test, 6, 60))\n",
    "\n",
    "dataset_test= TensorDataset(tensor_x_test,tensor_y_test)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True, drop_last= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4, 0, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_train = Path('/workspaces/betania/data/standartized_balanced/UCI/train.csv')\n",
    "\n",
    "y_train = pd.read_csv(data_path_train)\n",
    "\n",
    "y_train = y_train.iloc[:, -1]\n",
    "\n",
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accel-x-0</th>\n",
       "      <th>accel-x-1</th>\n",
       "      <th>accel-x-2</th>\n",
       "      <th>accel-x-3</th>\n",
       "      <th>accel-x-4</th>\n",
       "      <th>accel-x-5</th>\n",
       "      <th>accel-x-6</th>\n",
       "      <th>accel-x-7</th>\n",
       "      <th>accel-x-8</th>\n",
       "      <th>accel-x-9</th>\n",
       "      <th>...</th>\n",
       "      <th>gyro-z-50</th>\n",
       "      <th>gyro-z-51</th>\n",
       "      <th>gyro-z-52</th>\n",
       "      <th>gyro-z-53</th>\n",
       "      <th>gyro-z-54</th>\n",
       "      <th>gyro-z-55</th>\n",
       "      <th>gyro-z-56</th>\n",
       "      <th>gyro-z-57</th>\n",
       "      <th>gyro-z-58</th>\n",
       "      <th>gyro-z-59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.339089</td>\n",
       "      <td>-1.167666</td>\n",
       "      <td>-3.694124</td>\n",
       "      <td>-3.991698</td>\n",
       "      <td>-3.144751</td>\n",
       "      <td>-4.243802</td>\n",
       "      <td>-0.346311</td>\n",
       "      <td>4.784375</td>\n",
       "      <td>5.407283</td>\n",
       "      <td>2.445792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022887</td>\n",
       "      <td>-0.159737</td>\n",
       "      <td>-0.375233</td>\n",
       "      <td>0.051135</td>\n",
       "      <td>0.921111</td>\n",
       "      <td>0.278494</td>\n",
       "      <td>-0.133034</td>\n",
       "      <td>-0.033831</td>\n",
       "      <td>-0.145661</td>\n",
       "      <td>-0.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.999586</td>\n",
       "      <td>-1.566372</td>\n",
       "      <td>-1.517025</td>\n",
       "      <td>-2.713303</td>\n",
       "      <td>0.712069</td>\n",
       "      <td>2.309685</td>\n",
       "      <td>3.010014</td>\n",
       "      <td>3.875827</td>\n",
       "      <td>1.239790</td>\n",
       "      <td>-0.457387</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.684964</td>\n",
       "      <td>-0.555177</td>\n",
       "      <td>-0.076758</td>\n",
       "      <td>0.106422</td>\n",
       "      <td>0.348305</td>\n",
       "      <td>0.280755</td>\n",
       "      <td>0.141009</td>\n",
       "      <td>-0.206322</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.119462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.876184</td>\n",
       "      <td>-1.637183</td>\n",
       "      <td>-0.968435</td>\n",
       "      <td>-4.894519</td>\n",
       "      <td>-2.211239</td>\n",
       "      <td>-0.815775</td>\n",
       "      <td>1.349346</td>\n",
       "      <td>0.910528</td>\n",
       "      <td>1.072714</td>\n",
       "      <td>2.240573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064544</td>\n",
       "      <td>0.164004</td>\n",
       "      <td>0.180907</td>\n",
       "      <td>-0.684902</td>\n",
       "      <td>-0.689362</td>\n",
       "      <td>0.008271</td>\n",
       "      <td>0.051843</td>\n",
       "      <td>0.089196</td>\n",
       "      <td>-0.178239</td>\n",
       "      <td>0.031978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.649148</td>\n",
       "      <td>-1.854505</td>\n",
       "      <td>3.019927</td>\n",
       "      <td>3.966432</td>\n",
       "      <td>-0.782214</td>\n",
       "      <td>-2.535543</td>\n",
       "      <td>-3.642630</td>\n",
       "      <td>-2.326545</td>\n",
       "      <td>-2.164334</td>\n",
       "      <td>-0.037000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.264074</td>\n",
       "      <td>0.088056</td>\n",
       "      <td>0.465905</td>\n",
       "      <td>0.422788</td>\n",
       "      <td>-0.038715</td>\n",
       "      <td>-0.766346</td>\n",
       "      <td>0.376857</td>\n",
       "      <td>-0.127370</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.437026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.431428</td>\n",
       "      <td>0.846879</td>\n",
       "      <td>0.397590</td>\n",
       "      <td>1.620648</td>\n",
       "      <td>-2.014615</td>\n",
       "      <td>-3.534210</td>\n",
       "      <td>-1.916457</td>\n",
       "      <td>-1.600943</td>\n",
       "      <td>-1.037111</td>\n",
       "      <td>0.433378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246871</td>\n",
       "      <td>0.168610</td>\n",
       "      <td>0.257335</td>\n",
       "      <td>0.179625</td>\n",
       "      <td>0.129522</td>\n",
       "      <td>0.167631</td>\n",
       "      <td>-0.061982</td>\n",
       "      <td>0.597766</td>\n",
       "      <td>-0.117428</td>\n",
       "      <td>-0.170377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415</th>\n",
       "      <td>-0.055121</td>\n",
       "      <td>-0.013841</td>\n",
       "      <td>-0.015698</td>\n",
       "      <td>-0.032567</td>\n",
       "      <td>-0.020638</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>0.029549</td>\n",
       "      <td>0.035703</td>\n",
       "      <td>-0.009456</td>\n",
       "      <td>-0.038961</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005325</td>\n",
       "      <td>-0.000865</td>\n",
       "      <td>-0.006344</td>\n",
       "      <td>-0.000562</td>\n",
       "      <td>-0.011373</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>-0.001771</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>-0.000628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2416</th>\n",
       "      <td>-0.015348</td>\n",
       "      <td>-0.050589</td>\n",
       "      <td>-0.000972</td>\n",
       "      <td>0.005511</td>\n",
       "      <td>0.010061</td>\n",
       "      <td>0.016410</td>\n",
       "      <td>-0.041210</td>\n",
       "      <td>0.058844</td>\n",
       "      <td>-0.025887</td>\n",
       "      <td>-0.011735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018803</td>\n",
       "      <td>0.016867</td>\n",
       "      <td>0.012139</td>\n",
       "      <td>0.007154</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>-0.001346</td>\n",
       "      <td>-0.000821</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>-0.006642</td>\n",
       "      <td>-0.012614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>-0.003522</td>\n",
       "      <td>0.015157</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>-0.022950</td>\n",
       "      <td>-0.015183</td>\n",
       "      <td>-0.001276</td>\n",
       "      <td>0.010532</td>\n",
       "      <td>-0.017849</td>\n",
       "      <td>-0.042912</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.003882</td>\n",
       "      <td>0.006880</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.005425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>0.162229</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>-0.228693</td>\n",
       "      <td>-0.053321</td>\n",
       "      <td>0.088912</td>\n",
       "      <td>0.067402</td>\n",
       "      <td>-0.140253</td>\n",
       "      <td>-0.095661</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>-0.058650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007322</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>-0.003199</td>\n",
       "      <td>-0.009513</td>\n",
       "      <td>-0.007301</td>\n",
       "      <td>-0.003157</td>\n",
       "      <td>-0.001795</td>\n",
       "      <td>0.006901</td>\n",
       "      <td>-0.002802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>0.030335</td>\n",
       "      <td>-0.052862</td>\n",
       "      <td>-0.015650</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>-0.020507</td>\n",
       "      <td>0.048299</td>\n",
       "      <td>0.016255</td>\n",
       "      <td>-0.020065</td>\n",
       "      <td>0.015202</td>\n",
       "      <td>0.012079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>-0.007933</td>\n",
       "      <td>-0.004711</td>\n",
       "      <td>-0.013027</td>\n",
       "      <td>-0.013000</td>\n",
       "      <td>-0.009908</td>\n",
       "      <td>-0.009209</td>\n",
       "      <td>-0.009237</td>\n",
       "      <td>-0.017097</td>\n",
       "      <td>-0.003356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2420 rows × 360 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      accel-x-0  accel-x-1  accel-x-2  accel-x-3  accel-x-4  accel-x-5  \\\n",
       "0      4.339089  -1.167666  -3.694124  -3.991698  -3.144751  -4.243802   \n",
       "1     -1.999586  -1.566372  -1.517025  -2.713303   0.712069   2.309685   \n",
       "2     -0.876184  -1.637183  -0.968435  -4.894519  -2.211239  -0.815775   \n",
       "3      1.649148  -1.854505   3.019927   3.966432  -0.782214  -2.535543   \n",
       "4      3.431428   0.846879   0.397590   1.620648  -2.014615  -3.534210   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2415  -0.055121  -0.013841  -0.015698  -0.032567  -0.020638   0.020384   \n",
       "2416  -0.015348  -0.050589  -0.000972   0.005511   0.010061   0.016410   \n",
       "2417  -0.003522   0.015157   0.029197  -0.022950  -0.015183  -0.001276   \n",
       "2418   0.162229   0.062337  -0.228693  -0.053321   0.088912   0.067402   \n",
       "2419   0.030335  -0.052862  -0.015650   0.001528  -0.020507   0.048299   \n",
       "\n",
       "      accel-x-6  accel-x-7  accel-x-8  accel-x-9  ...  gyro-z-50  gyro-z-51  \\\n",
       "0     -0.346311   4.784375   5.407283   2.445792  ...   0.022887  -0.159737   \n",
       "1      3.010014   3.875827   1.239790  -0.457387  ...  -0.684964  -0.555177   \n",
       "2      1.349346   0.910528   1.072714   2.240573  ...   0.064544   0.164004   \n",
       "3     -3.642630  -2.326545  -2.164334  -0.037000  ...  -0.264074   0.088056   \n",
       "4     -1.916457  -1.600943  -1.037111   0.433378  ...   0.246871   0.168610   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2415   0.029549   0.035703  -0.009456  -0.038961  ...  -0.005325  -0.000865   \n",
       "2416  -0.041210   0.058844  -0.025887  -0.011735  ...   0.018803   0.016867   \n",
       "2417   0.010532  -0.017849  -0.042912   0.004876  ...   0.001644   0.000004   \n",
       "2418  -0.140253  -0.095661   0.138672  -0.058650  ...   0.007322   0.005421   \n",
       "2419   0.016255  -0.020065   0.015202   0.012079  ...   0.005493  -0.007933   \n",
       "\n",
       "      gyro-z-52  gyro-z-53  gyro-z-54  gyro-z-55  gyro-z-56  gyro-z-57  \\\n",
       "0     -0.375233   0.051135   0.921111   0.278494  -0.133034  -0.033831   \n",
       "1     -0.076758   0.106422   0.348305   0.280755   0.141009  -0.206322   \n",
       "2      0.180907  -0.684902  -0.689362   0.008271   0.051843   0.089196   \n",
       "3      0.465905   0.422788  -0.038715  -0.766346   0.376857  -0.127370   \n",
       "4      0.257335   0.179625   0.129522   0.167631  -0.061982   0.597766   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2415  -0.006344  -0.000562  -0.011373  -0.000161   0.007937  -0.001771   \n",
       "2416   0.012139   0.007154   0.008134  -0.001346  -0.000821   0.003322   \n",
       "2417   0.003882   0.006880   0.001106   0.005042   0.008657   0.002099   \n",
       "2418   0.004941  -0.003199  -0.009513  -0.007301  -0.003157  -0.001795   \n",
       "2419  -0.004711  -0.013027  -0.013000  -0.009908  -0.009209  -0.009237   \n",
       "\n",
       "      gyro-z-58  gyro-z-59  \n",
       "0     -0.145661  -0.039100  \n",
       "1      0.010000   0.119462  \n",
       "2     -0.178239   0.031978  \n",
       "3      0.074219   0.437026  \n",
       "4     -0.117428  -0.170377  \n",
       "...         ...        ...  \n",
       "2415  -0.000563  -0.000628  \n",
       "2416  -0.006642  -0.012614  \n",
       "2417   0.008879   0.005425  \n",
       "2418   0.006901  -0.002802  \n",
       "2419  -0.017097  -0.003356  \n",
       "\n",
       "[2420 rows x 360 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 shape: torch.Size([32, 6, 60])\n",
      "Batch 1 shape: torch.Size([32, 6, 60])\n",
      "Batch 2 shape: torch.Size([32, 6, 60])\n",
      "Batch 3 shape: torch.Size([32, 6, 60])\n",
      "Batch 4 shape: torch.Size([32, 6, 60])\n",
      "Batch 5 shape: torch.Size([32, 6, 60])\n",
      "Batch 6 shape: torch.Size([32, 6, 60])\n",
      "Batch 7 shape: torch.Size([32, 6, 60])\n",
      "Batch 8 shape: torch.Size([32, 6, 60])\n",
      "Batch 9 shape: torch.Size([32, 6, 60])\n",
      "Batch 10 shape: torch.Size([32, 6, 60])\n",
      "Batch 11 shape: torch.Size([32, 6, 60])\n",
      "Batch 12 shape: torch.Size([32, 6, 60])\n",
      "Batch 13 shape: torch.Size([32, 6, 60])\n",
      "Batch 14 shape: torch.Size([32, 6, 60])\n",
      "Batch 15 shape: torch.Size([32, 6, 60])\n",
      "Batch 16 shape: torch.Size([32, 6, 60])\n",
      "Batch 17 shape: torch.Size([32, 6, 60])\n",
      "Batch 18 shape: torch.Size([32, 6, 60])\n",
      "Batch 19 shape: torch.Size([32, 6, 60])\n",
      "Batch 20 shape: torch.Size([32, 6, 60])\n",
      "Batch 21 shape: torch.Size([32, 6, 60])\n",
      "Batch 22 shape: torch.Size([32, 6, 60])\n",
      "Batch 23 shape: torch.Size([32, 6, 60])\n",
      "Batch 24 shape: torch.Size([32, 6, 60])\n",
      "Batch 25 shape: torch.Size([32, 6, 60])\n",
      "Batch 26 shape: torch.Size([32, 6, 60])\n",
      "Batch 27 shape: torch.Size([32, 6, 60])\n",
      "Batch 28 shape: torch.Size([32, 6, 60])\n",
      "Batch 29 shape: torch.Size([32, 6, 60])\n",
      "Batch 30 shape: torch.Size([32, 6, 60])\n",
      "Batch 31 shape: torch.Size([32, 6, 60])\n",
      "Batch 32 shape: torch.Size([32, 6, 60])\n",
      "Batch 33 shape: torch.Size([32, 6, 60])\n",
      "Batch 34 shape: torch.Size([32, 6, 60])\n",
      "Batch 35 shape: torch.Size([32, 6, 60])\n",
      "Batch 36 shape: torch.Size([32, 6, 60])\n",
      "Batch 37 shape: torch.Size([32, 6, 60])\n",
      "Batch 38 shape: torch.Size([32, 6, 60])\n",
      "Batch 39 shape: torch.Size([32, 6, 60])\n",
      "Batch 40 shape: torch.Size([32, 6, 60])\n",
      "Batch 41 shape: torch.Size([32, 6, 60])\n",
      "Batch 42 shape: torch.Size([32, 6, 60])\n",
      "Batch 43 shape: torch.Size([32, 6, 60])\n",
      "Batch 44 shape: torch.Size([32, 6, 60])\n",
      "Batch 45 shape: torch.Size([32, 6, 60])\n",
      "Batch 46 shape: torch.Size([32, 6, 60])\n",
      "Batch 47 shape: torch.Size([32, 6, 60])\n",
      "Batch 48 shape: torch.Size([32, 6, 60])\n",
      "Batch 49 shape: torch.Size([32, 6, 60])\n",
      "Batch 50 shape: torch.Size([32, 6, 60])\n",
      "Batch 51 shape: torch.Size([32, 6, 60])\n",
      "Batch 52 shape: torch.Size([32, 6, 60])\n",
      "Batch 53 shape: torch.Size([32, 6, 60])\n",
      "Batch 54 shape: torch.Size([32, 6, 60])\n",
      "Batch 55 shape: torch.Size([32, 6, 60])\n",
      "Batch 56 shape: torch.Size([32, 6, 60])\n",
      "Batch 57 shape: torch.Size([32, 6, 60])\n",
      "Batch 58 shape: torch.Size([32, 6, 60])\n",
      "Batch 59 shape: torch.Size([32, 6, 60])\n",
      "Batch 60 shape: torch.Size([32, 6, 60])\n",
      "Batch 61 shape: torch.Size([32, 6, 60])\n",
      "Batch 62 shape: torch.Size([32, 6, 60])\n",
      "Batch 63 shape: torch.Size([32, 6, 60])\n",
      "Batch 64 shape: torch.Size([32, 6, 60])\n",
      "Batch 65 shape: torch.Size([32, 6, 60])\n",
      "Batch 66 shape: torch.Size([32, 6, 60])\n",
      "Batch 67 shape: torch.Size([32, 6, 60])\n",
      "Batch 68 shape: torch.Size([32, 6, 60])\n",
      "Batch 69 shape: torch.Size([32, 6, 60])\n",
      "Batch 70 shape: torch.Size([32, 6, 60])\n",
      "Batch 71 shape: torch.Size([32, 6, 60])\n",
      "Batch 72 shape: torch.Size([32, 6, 60])\n",
      "Batch 73 shape: torch.Size([32, 6, 60])\n",
      "Batch 74 shape: torch.Size([32, 6, 60])\n",
      "Batch 0 shape: torch.Size([32])\n",
      "Batch 1 shape: torch.Size([32])\n",
      "Batch 2 shape: torch.Size([32])\n",
      "Batch 3 shape: torch.Size([32])\n",
      "Batch 4 shape: torch.Size([32])\n",
      "Batch 5 shape: torch.Size([32])\n",
      "Batch 6 shape: torch.Size([32])\n",
      "Batch 7 shape: torch.Size([32])\n",
      "Batch 8 shape: torch.Size([32])\n",
      "Batch 9 shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# shape dataloader\n",
    "\n",
    "for i, (x, y) in enumerate(dataloader_train):\n",
    "    print(f\"Batch {i} shape: {x.shape}\")\n",
    "\n",
    "for i, (x, y) in enumerate(dataloader_val):\n",
    "    print(f\"Batch {i} shape: {y.shape}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding_size = 150\n",
    "# n_classes = 6\n",
    "\n",
    "# classifier = StateClassifier(input_size=encoding_size, num_classes=n_classes, dropout_prob=0, hidden_size1= 64, hidden_size2=32)\n",
    "# cpc_classifier = CPC_Classifier(encoder, classifier)\n",
    "\n",
    "encoding_size = 256\n",
    "\n",
    "n_classes = 7\n",
    "\n",
    "classifier = StateClassifier(input_size=encoding_size, n_classes=n_classes, dropout_prob=0, hidden_size1= 64, hidden_size2=32)\n",
    "\n",
    "cpc = CPC(encoderCNN, encoder, density_estimator, auto_regressor)\n",
    "\n",
    "cpc_classifier = CPC_Classifier(encoderCNN, encoder, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpc_classifier = cpc_classifier.to('cpu')\n",
    "\n",
    "# xtest = cpc_classifier.forward(tensor_x_test.to('cpu'))\n",
    "\n",
    "# xtest = xtest.detach().numpy()\n",
    "\n",
    "# xtrain = cpc_classifier.forward(tensor_x.to('cpu'))\n",
    "\n",
    "# xtrain = xtrain.detach().numpy()\n",
    "\n",
    "# ytrain = tensor_y.detach().numpy()\n",
    "\n",
    "# ytest = tensor_y_test.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# random_forest_model.fit(xtrain, ytrain)\n",
    "\n",
    "# # Faça previsões no conjunto de teste\n",
    "# predictions = random_forest_model.predict(xtest)\n",
    "\n",
    "# # Avalie o desempenho do modelo usando métricas, como a precisão\n",
    "# accuracy = accuracy_score(ytest, predictions)\n",
    "\n",
    "# print(f'A precisão do modelo no conjunto de teste é: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KNN\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Criar o modelo KNN\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=5)  # Pode ajustar o número de vizinhos conforme necessário\n",
    "\n",
    "# # Treinar o modelo\n",
    "# knn_model.fit(xtrain, ytrain)\n",
    "\n",
    "# # Fazer previsões no conjunto de teste\n",
    "# predictions_knn = knn_model.predict(xtest)\n",
    "\n",
    "# # Avaliar o desempenho do modelo usando métricas, como a precisão\n",
    "# accuracy_knn = accuracy_score(ytest, predictions_knn)\n",
    "\n",
    "# print(\"Accuracy (KNN):\", accuracy_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Criar o modelo SVM\n",
    "# svm_model = SVC(kernel='linear', C=1.0)  # Pode ajustar o kernel e C conforme necessário\n",
    "\n",
    "# # Treinar o modelo\n",
    "# svm_model.fit(xtrain, ytrain)\n",
    "\n",
    "# # Fazer previsões no conjunto de teste\n",
    "# predictions_svm = svm_model.predict(xtest)\n",
    "\n",
    "# # Avaliar o desempenho do modelo usando métricas, como a precisão\n",
    "# accuracy_svm = accuracy_score(ytest, predictions_svm)\n",
    "\n",
    "# print(\"Accuracy (SVM):\", accuracy_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1000, accelerator=\"gpu\", devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | encoderCNN    | CNNEncoder       | 31.5 K\n",
      "1 | encoder       | GRUEncoder       | 189 K \n",
      "2 | classifier    | StateClassifier  | 18.8 K\n",
      "3 | loss_function | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "239 K     Trainable params\n",
      "0         Non-trainable params\n",
      "239 K     Total params\n",
      "0.959     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "/home/vscode/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:309: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:  44%|████▍     | 33/75 [00:00<00:00, 71.89it/s, v_num=184, val_loss=1.640, train_loss=1.640] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(cpc_classifier, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 21/21 [00:00<00:00, 82.85it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.6592261791229248\n",
      "        test_loss           1.4835978746414185\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.6592261791229248, 'test_loss': 1.4835978746414185}]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(cpc_classifier, dataloader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
