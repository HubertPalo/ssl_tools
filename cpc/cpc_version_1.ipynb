{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando bibliotecas\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPC SSL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de Treino\n",
    "\n",
    "data_path = Path('/workspaces/betania/ssl_tools/view_concatenated/WISDM_cpc/train')\n",
    "\n",
    "datas_x_train = []\n",
    "\n",
    "data_y_train = []\n",
    "\n",
    "for f in data_path.glob('*.csv'):\n",
    "    data = pd.read_csv(f)\n",
    "    x = data[['accel-x', 'accel-y', 'accel-z', 'gyro-x', 'gyro-y', 'gyro-z']].values\n",
    "\n",
    "    # Expend dimension\n",
    "\n",
    "    x = np.swapaxes(x, 1, 0)\n",
    "    \n",
    "    datas_x_train.append(x)\n",
    "\n",
    "    y = data['standard activity code'].values\n",
    "\n",
    "    data_y_train.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array 1: Shape (6, 17969)\n",
      "Array 2: Shape (6, 17974)\n",
      "Array 3: Shape (6, 17994)\n",
      "Array 4: Shape (6, 17965)\n",
      "Array 5: Shape (6, 17991)\n",
      "Array 6: Shape (6, 17964)\n",
      "Array 7: Shape (6, 17982)\n",
      "Array 8: Shape (6, 35935)\n",
      "Array 9: Shape (6, 17964)\n",
      "Array 10: Shape (6, 17983)\n",
      "Array 11: Shape (6, 17964)\n",
      "Array 12: Shape (6, 17967)\n",
      "Array 13: Shape (6, 17994)\n",
      "Array 14: Shape (6, 17966)\n",
      "Array 15: Shape (6, 17982)\n",
      "Array 16: Shape (6, 17991)\n",
      "Array 17: Shape (6, 17966)\n",
      "Array 18: Shape (6, 17966)\n",
      "Array 19: Shape (6, 17965)\n",
      "Array 20: Shape (6, 17963)\n",
      "Array 21: Shape (6, 17967)\n",
      "Array 22: Shape (6, 17983)\n",
      "Array 23: Shape (6, 17964)\n",
      "Array 24: Shape (6, 17972)\n",
      "Array 25: Shape (6, 17984)\n",
      "Array 26: Shape (6, 17964)\n",
      "Array 27: Shape (6, 17967)\n",
      "Array 28: Shape (6, 17965)\n",
      "Array 29: Shape (6, 17995)\n",
      "Array 30: Shape (6, 17963)\n",
      "Array 31: Shape (6, 17982)\n",
      "Array 32: Shape (6, 17985)\n",
      "Array 33: Shape (6, 17981)\n",
      "Array 34: Shape (6, 17970)\n",
      "Array 35: Shape (6, 17968)\n",
      "Array 36: Shape (6, 17965)\n"
     ]
    }
   ],
   "source": [
    "# Use a função shape para obter o shape de cada array na lista\n",
    "formas = [arr.shape for arr in datas_x_train]\n",
    "\n",
    "# Isso irá imprimir as formas (shapes) de cada array na lista\n",
    "for i, forma in enumerate(formas):\n",
    "    print(f\"Array {i+1}: Shape {forma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de Teste\n",
    "\n",
    "data_path = Path('/workspaces/betania/ssl_tools/view_concatenated/WISDM_cpc/test')\n",
    "\n",
    "datas_x_test = []\n",
    "\n",
    "data_y_test = []\n",
    "\n",
    "for f in data_path.glob('*.csv'):\n",
    "    data = pd.read_csv(f)\n",
    "    x = data[['accel-x', 'accel-y', 'accel-z', 'gyro-x', 'gyro-y', 'gyro-z']].values\n",
    "\n",
    "    # Expend dimension\n",
    "\n",
    "    x = np.swapaxes(x, 1, 0)\n",
    "    \n",
    "    datas_x_test.append(x)\n",
    "\n",
    "    y = data['standard activity code'].values\n",
    "\n",
    "    data_y_test.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = data['standard activity code'].values\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = datas_x_train\n",
    "y_train = data_y_train\n",
    "x_test = datas_x_test\n",
    "y_test = data_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2, 2, 2, ..., 1, 1, 1]),\n",
       " array([2, 2, 2, ..., 1, 1, 1]),\n",
       " array([2, 2, 2, ..., 1, 1, 1]),\n",
       " array([2, 2, 2, ..., 1, 1, 1]),\n",
       " array([2, 2, 2, ..., 1, 1, 1]),\n",
       " array([2, 2, 2, ..., 1, 1, 1]),\n",
       " array([2, 2, 2, ..., 1, 1, 1]),\n",
       " array([2, 2, 2, ..., 1, 1, 1]),\n",
       " array([2, 2, 2, ..., 1, 1, 1]),\n",
       " array([2, 2, 2, ..., 1, 1, 1]),\n",
       " array([2, 2, 2, ..., 1, 1, 1])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume todos os vetores z em em único vetor de contexto​\n",
    "\n",
    "class GRUEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 100,\n",
    "        in_channel: int = 6,\n",
    "        encoding_size: int = 10,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        bidirectional: bool = True,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_channel = in_channel\n",
    "        self.num_layers = num_layers\n",
    "        self.encoding_size = encoding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.device = device\n",
    "        \n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=self.in_channel,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "        ).to(device) \n",
    "\n",
    "        self.nn = torch.nn.Linear(\n",
    "            self.hidden_size * (int(self.bidirectional) + 1), self.encoding_size\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)\n",
    "\n",
    "        past = torch.zeros(\n",
    "            self.num_layers * (int(self.bidirectional) + 1),\n",
    "            x.shape[1],\n",
    "            self.hidden_size,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        out, _ = self.rnn(\n",
    "            x, past\n",
    "        )  # out shape = [seq_len, batch_size, num_directions*hidden_size]\n",
    "        encodings = self.nn(out[-1].squeeze(0))\n",
    "        return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: torch.nn.Module,\n",
    "        density_estimator: torch.nn.Module,\n",
    "        auto_regressor: torch.nn.Module,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 0.0,\n",
    "        window_size: int = 4,\n",
    "        n_size: int = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.density_estimator = density_estimator.to(self.device)\n",
    "        self.auto_regressor = auto_regressor.to(self.device)\n",
    "        self.learning_rate = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.window_size = window_size\n",
    "        self.n_size = n_size\n",
    "        self.training_step_losses = []\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        assert len(batch) == 1, \"Batch must be 1 sample only\"\n",
    "        sample = batch\n",
    "        sample = sample.squeeze(0)\n",
    "        rnd_t = np.random.randint(\n",
    "            5 * self.window_size, sample.shape[-1] - 5 * self.window_size\n",
    "        )\n",
    "        sample = torch.tensor(\n",
    "            sample[\n",
    "                :,\n",
    "                max(0, (rnd_t - 20 * self.window_size)) : min(\n",
    "                    sample.shape[-1], rnd_t + 20 * self.window_size\n",
    "                ),\n",
    "            ]\n",
    "        ).cpu()\n",
    "\n",
    "        T = sample.shape[-1]\n",
    "        windowed_sample = np.split(\n",
    "            sample[:, : (T // self.window_size) * self.window_size],\n",
    "            (T // self.window_size),\n",
    "            -1,\n",
    "        )\n",
    "        windowed_sample = torch.tensor(np.stack(windowed_sample, 0), device=self.device)\n",
    "        encodings = self.encoder(windowed_sample)\n",
    "        window_ind = torch.randint(2, len(encodings) - 2, size=(1,))\n",
    "        _, c_t = self.auto_regressor(\n",
    "            encodings[max(0, window_ind[0] - 10) : window_ind[0] + 1].unsqueeze(0)\n",
    "        )\n",
    "        density_ratios = torch.bmm(\n",
    "            encodings.unsqueeze(1),\n",
    "            self.density_estimator(c_t.squeeze(1).squeeze(0)).expand_as(encodings).unsqueeze(-1),\n",
    "        ).view(\n",
    "            -1,\n",
    "        )\n",
    "        r = set(range(0, window_ind[0] - 2))\n",
    "        r.update(set(range(window_ind[0] + 3, len(encodings))))\n",
    "        rnd_n = np.random.choice(list(r), self.n_size)\n",
    "        X_N = torch.cat(\n",
    "            [density_ratios[rnd_n], density_ratios[window_ind[0] + 1].unsqueeze(0)], 0\n",
    "        )\n",
    "        labels = torch.Tensor([len(X_N) - 1]).to(self.device)\n",
    "        loss = torch.nn.CrossEntropyLoss()(X_N.view(1, -1), labels.long())\n",
    "        self.training_step_losses.append(loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.training_step_losses).mean()\n",
    "        self.log(\"train_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        # free up the memory\n",
    "        self.training_step_losses.clear()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        learnable_parameters = (\n",
    "            list(self.density_estimator.parameters())\n",
    "            + list(self.encoder.parameters())\n",
    "            + list(self.auto_regressor.parameters())\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            learnable_parameters, lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_size = 256\n",
    "encoder = GRUEncoder(encoding_size=256, device='cuda')\n",
    "density_estimator = torch.nn.Linear(encoding_size, encoding_size)\n",
    "auto_regressor = torch.nn.GRU(\n",
    "    input_size=encoding_size, \n",
    "    hidden_size=encoding_size, \n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "cpc = CPC(encoder, density_estimator, auto_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 11)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleDataset:\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx].astype(np.float32), self.y[idx].astype(np.float32)\n",
    "        else:\n",
    "            return self.X[idx].astype(np.float32)\n",
    "    \n",
    "train_dataset = SimpleDataset(x_train)\n",
    "test_dataset = SimpleDataset(x_test, y_test)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=500, accelerator=\"gpu\", devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name              | Type       | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder           | GRUEncoder | 116 K \n",
      "1 | density_estimator | Linear     | 65.8 K\n",
      "2 | auto_regressor    | GRU        | 394 K \n",
      "-------------------------------------------------\n",
      "576 K     Trainable params\n",
      "0         Non-trainable params\n",
      "576 K     Total params\n",
      "2.307     Total estimated model params size (MB)\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  69%|██████▉   | 25/36 [00:00<00:00, 126.30it/s, v_num=136]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_280400/3311096431.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/36 [00:00<?, ?it/s, v_num=136]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499: 100%|██████████| 36/36 [00:00<00:00, 50.96it/s, v_num=136, train_loss=1.740]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499: 100%|██████████| 36/36 [00:00<00:00, 42.40it/s, v_num=136, train_loss=1.740]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(cpc, train_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPC Fine-Tuning\n",
    "\n",
    "We are going to fine-tune the CPC model on the downstream task of classification. We will use the same dataset and re-use the same encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "\n",
    "class StateClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size: int = 10, n_classes: int = 7):\n",
    "        super(StateClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.n_classes = 7\n",
    "        self.normalize = torch.nn.BatchNorm1d(self.input_size)\n",
    "        self.nn = torch.nn.Linear(self.input_size, self.n_classes)\n",
    "        torch.nn.init.xavier_uniform_(self.nn.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.normalize(x)\n",
    "        logits = self.nn(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class CPC_Classifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: torch.nn.Module,\n",
    "        classifier: torch.nn.Module,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 0.0,\n",
    "        task_class: str = \"multiclass\",\n",
    "        num_classes: int = 7\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.classifier = classifier.to(self.device)\n",
    "        self.learning_rate = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.training_step_losses = []\n",
    "        self.validation_step_losses = []\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        self.task_class = task_class\n",
    "        self.num_classes = 7\n",
    "        \n",
    "    def configure_optimizers(self) -> Any:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.classifier.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encodings = self.encoder(x)\n",
    "        predictions = self.classifier(encodings)\n",
    "        return predictions\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        predictions = self.forward(x)\n",
    "        loss = self.loss_function(predictions, y.long())\n",
    "        self.training_step_losses.append(loss)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.training_step_losses).mean()\n",
    "        self.log(\"train_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        # free up the memory\n",
    "        self.training_step_losses.clear()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        self.validation_step_losses.append(loss)\n",
    "        metrics = {\"val_acc\": acc, \"val_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "        \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.validation_step_losses).mean()\n",
    "        self.log(\"val_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        # free up the memory\n",
    "        self.validation_step_losses.clear()\n",
    "\n",
    "    def _shared_eval_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        predictions = self.forward(x)\n",
    "        loss = self.loss_function(predictions, y.long())\n",
    "        acc = accuracy(torch.argmax(predictions, dim=1), y.long(), task=self.task_class, num_classes=self.num_classes)\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Caminho dos dados\n",
    "\n",
    "data_path_train = Path('/workspaces/betania/data/standartized_balanced/WISDM/train.csv')\n",
    "\n",
    "data_path_validation = Path('/workspaces/betania/data/standartized_balanced/WISDM/validation.csv')\n",
    "\n",
    "data_path_test = Path('/workspaces/betania/data/standartized_balanced/WISDM/test.csv')\n",
    "\n",
    "# Train\n",
    "\n",
    "x_train = pd.read_csv(data_path_train)\n",
    "\n",
    "x_train = x_train.iloc[:, :360]\n",
    "    \n",
    "x_train = x_train.astype(np.float32)\n",
    "\n",
    "y_train = pd.read_csv(data_path_train)\n",
    "\n",
    "y_train = y_train.iloc[:, -1]\n",
    "\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "tensor_x = torch.Tensor(np.array(x_train))\n",
    "\n",
    "tensor_y = torch.Tensor(np.array(y_train))\n",
    "\n",
    "original_dim = tensor_x.shape[0]\n",
    "\n",
    "input_shape = (original_dim, 6, 60)\n",
    "\n",
    "tensor_x = tensor_x.reshape(input_shape)\n",
    "\n",
    "dataset_train= TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last= True)\n",
    "\n",
    "# Validation\n",
    "\n",
    "x_validation = pd.read_csv(data_path_validation)\n",
    "\n",
    "x_validation = x_validation.iloc[:, :360]\n",
    "\n",
    "x_validation = x_validation.astype(np.float32)\n",
    "\n",
    "y_validation = pd.read_csv(data_path_validation)\n",
    "\n",
    "y_validation = y_validation.iloc[:, -1]\n",
    "\n",
    "y_validation = y_validation.astype(np.float32)\n",
    "\n",
    "tensor_x_val = torch.Tensor(np.array(x_validation))\n",
    "\n",
    "tensor_y_val = torch.Tensor(np.array(y_validation))\n",
    "\n",
    "original_dim_val = tensor_x_val.shape[0]\n",
    "\n",
    "tensor_x_val = tensor_x_val.reshape(original_dim_val, 6, 60)\n",
    "\n",
    "dataset_val= TensorDataset(tensor_x_val,tensor_y_val)\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "\n",
    "# Test\n",
    "\n",
    "x_test = pd.read_csv(data_path_test)\n",
    "\n",
    "x_test = x_test.iloc[:, :360]\n",
    "\n",
    "x_test = x_test.astype(np.float32)\n",
    "\n",
    "y_test = pd.read_csv(data_path_test)\n",
    "\n",
    "y_test = y_test.iloc[:, -1]\n",
    "\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "tensor_x_test = torch.Tensor(np.array(x_test))\n",
    "\n",
    "tensor_y_test = torch.Tensor(np.array(y_test))\n",
    "\n",
    "original_dim_test = tensor_x_test.shape[0]\n",
    "\n",
    "tensor_x_test = tensor_x_test.reshape((original_dim_test, 6, 60))\n",
    "\n",
    "dataset_test= TensorDataset(tensor_x_test,tensor_y_test)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True, drop_last= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 6, 0, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_train = Path('/workspaces/betania/data/standartized_balanced/WISDM/train.csv')\n",
    "\n",
    "y_train = pd.read_csv(data_path_train)\n",
    "\n",
    "y_train = y_train.iloc[:, -1]\n",
    "\n",
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accel-x-0</th>\n",
       "      <th>accel-x-1</th>\n",
       "      <th>accel-x-2</th>\n",
       "      <th>accel-x-3</th>\n",
       "      <th>accel-x-4</th>\n",
       "      <th>accel-x-5</th>\n",
       "      <th>accel-x-6</th>\n",
       "      <th>accel-x-7</th>\n",
       "      <th>accel-x-8</th>\n",
       "      <th>accel-x-9</th>\n",
       "      <th>...</th>\n",
       "      <th>gyro-z-50</th>\n",
       "      <th>gyro-z-51</th>\n",
       "      <th>gyro-z-52</th>\n",
       "      <th>gyro-z-53</th>\n",
       "      <th>gyro-z-54</th>\n",
       "      <th>gyro-z-55</th>\n",
       "      <th>gyro-z-56</th>\n",
       "      <th>gyro-z-57</th>\n",
       "      <th>gyro-z-58</th>\n",
       "      <th>gyro-z-59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.406084</td>\n",
       "      <td>-7.971005</td>\n",
       "      <td>-0.280978</td>\n",
       "      <td>8.237112</td>\n",
       "      <td>2.229308</td>\n",
       "      <td>-1.877797</td>\n",
       "      <td>-3.078670</td>\n",
       "      <td>-6.051075</td>\n",
       "      <td>-3.491006</td>\n",
       "      <td>-1.464815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.776451</td>\n",
       "      <td>-0.576500</td>\n",
       "      <td>-0.575899</td>\n",
       "      <td>-0.799183</td>\n",
       "      <td>-0.084052</td>\n",
       "      <td>0.752021</td>\n",
       "      <td>1.587773</td>\n",
       "      <td>2.650263</td>\n",
       "      <td>1.906534</td>\n",
       "      <td>0.092092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.186370</td>\n",
       "      <td>-0.050579</td>\n",
       "      <td>-0.040649</td>\n",
       "      <td>-0.449850</td>\n",
       "      <td>-0.655638</td>\n",
       "      <td>1.033459</td>\n",
       "      <td>0.007363</td>\n",
       "      <td>-3.121700</td>\n",
       "      <td>2.422174</td>\n",
       "      <td>-2.430532</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143452</td>\n",
       "      <td>-0.646708</td>\n",
       "      <td>0.207145</td>\n",
       "      <td>0.702881</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.190366</td>\n",
       "      <td>-0.486779</td>\n",
       "      <td>-0.079197</td>\n",
       "      <td>0.471224</td>\n",
       "      <td>-0.197064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.524070</td>\n",
       "      <td>0.704957</td>\n",
       "      <td>-7.538637</td>\n",
       "      <td>-8.366249</td>\n",
       "      <td>-0.896879</td>\n",
       "      <td>-2.866145</td>\n",
       "      <td>4.361573</td>\n",
       "      <td>3.237100</td>\n",
       "      <td>3.697426</td>\n",
       "      <td>3.434555</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.438400</td>\n",
       "      <td>-1.782263</td>\n",
       "      <td>-1.537537</td>\n",
       "      <td>-1.536583</td>\n",
       "      <td>-1.961644</td>\n",
       "      <td>-1.557289</td>\n",
       "      <td>0.286954</td>\n",
       "      <td>4.903331</td>\n",
       "      <td>5.336706</td>\n",
       "      <td>3.811437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.417248</td>\n",
       "      <td>-0.462334</td>\n",
       "      <td>-1.679691</td>\n",
       "      <td>-1.038921</td>\n",
       "      <td>-1.575214</td>\n",
       "      <td>0.749882</td>\n",
       "      <td>4.269343</td>\n",
       "      <td>3.831115</td>\n",
       "      <td>0.655725</td>\n",
       "      <td>-1.144818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189591</td>\n",
       "      <td>0.444646</td>\n",
       "      <td>0.451423</td>\n",
       "      <td>0.160605</td>\n",
       "      <td>-0.055797</td>\n",
       "      <td>-0.008116</td>\n",
       "      <td>0.130876</td>\n",
       "      <td>0.558405</td>\n",
       "      <td>0.738815</td>\n",
       "      <td>0.314354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.675334</td>\n",
       "      <td>-4.063461</td>\n",
       "      <td>-3.253331</td>\n",
       "      <td>5.513613</td>\n",
       "      <td>8.510528</td>\n",
       "      <td>8.789780</td>\n",
       "      <td>9.061282</td>\n",
       "      <td>2.349534</td>\n",
       "      <td>-10.563099</td>\n",
       "      <td>-8.488220</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.874475</td>\n",
       "      <td>0.041224</td>\n",
       "      <td>0.816235</td>\n",
       "      <td>0.905279</td>\n",
       "      <td>0.424946</td>\n",
       "      <td>-1.133572</td>\n",
       "      <td>-1.697517</td>\n",
       "      <td>-1.766260</td>\n",
       "      <td>-2.217915</td>\n",
       "      <td>-2.309474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10915</th>\n",
       "      <td>0.035006</td>\n",
       "      <td>0.073410</td>\n",
       "      <td>-0.033312</td>\n",
       "      <td>-0.036785</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>-0.008284</td>\n",
       "      <td>-0.019827</td>\n",
       "      <td>-0.037937</td>\n",
       "      <td>-0.048616</td>\n",
       "      <td>-0.003029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>-0.004577</td>\n",
       "      <td>-0.002461</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>-0.001429</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>-0.000314</td>\n",
       "      <td>0.004041</td>\n",
       "      <td>0.006878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10916</th>\n",
       "      <td>0.130879</td>\n",
       "      <td>0.216577</td>\n",
       "      <td>0.271304</td>\n",
       "      <td>0.435954</td>\n",
       "      <td>0.347626</td>\n",
       "      <td>-0.456578</td>\n",
       "      <td>-0.295231</td>\n",
       "      <td>-0.161805</td>\n",
       "      <td>-1.112699</td>\n",
       "      <td>-0.444722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007144</td>\n",
       "      <td>0.007289</td>\n",
       "      <td>-0.001360</td>\n",
       "      <td>-0.006746</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>-0.003875</td>\n",
       "      <td>-0.004737</td>\n",
       "      <td>-0.000545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10917</th>\n",
       "      <td>0.023335</td>\n",
       "      <td>0.013198</td>\n",
       "      <td>-0.008005</td>\n",
       "      <td>-0.012395</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>0.006365</td>\n",
       "      <td>0.030876</td>\n",
       "      <td>0.032646</td>\n",
       "      <td>0.032760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015202</td>\n",
       "      <td>-0.002399</td>\n",
       "      <td>0.012727</td>\n",
       "      <td>0.028815</td>\n",
       "      <td>0.037020</td>\n",
       "      <td>0.045893</td>\n",
       "      <td>0.046907</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.030555</td>\n",
       "      <td>-0.007662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10918</th>\n",
       "      <td>0.034519</td>\n",
       "      <td>-0.001241</td>\n",
       "      <td>-0.061202</td>\n",
       "      <td>-0.021307</td>\n",
       "      <td>0.023097</td>\n",
       "      <td>0.009323</td>\n",
       "      <td>0.012766</td>\n",
       "      <td>0.045946</td>\n",
       "      <td>-0.063485</td>\n",
       "      <td>-0.030942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001505</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>-0.013288</td>\n",
       "      <td>-0.003919</td>\n",
       "      <td>-0.007930</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>-0.001422</td>\n",
       "      <td>-0.004019</td>\n",
       "      <td>-0.002352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10919</th>\n",
       "      <td>0.048776</td>\n",
       "      <td>0.088065</td>\n",
       "      <td>0.111590</td>\n",
       "      <td>0.075649</td>\n",
       "      <td>0.019448</td>\n",
       "      <td>0.054759</td>\n",
       "      <td>0.160495</td>\n",
       "      <td>0.079276</td>\n",
       "      <td>-0.195114</td>\n",
       "      <td>-0.303471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002672</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>-0.004062</td>\n",
       "      <td>-0.007611</td>\n",
       "      <td>-0.008637</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.010109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10920 rows × 360 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       accel-x-0  accel-x-1  accel-x-2  accel-x-3  accel-x-4  accel-x-5  \\\n",
       "0      -4.406084  -7.971005  -0.280978   8.237112   2.229308  -1.877797   \n",
       "1       0.186370  -0.050579  -0.040649  -0.449850  -0.655638   1.033459   \n",
       "2       5.524070   0.704957  -7.538637  -8.366249  -0.896879  -2.866145   \n",
       "3      -1.417248  -0.462334  -1.679691  -1.038921  -1.575214   0.749882   \n",
       "4      -6.675334  -4.063461  -3.253331   5.513613   8.510528   8.789780   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "10915   0.035006   0.073410  -0.033312  -0.036785   0.077832  -0.008284   \n",
       "10916   0.130879   0.216577   0.271304   0.435954   0.347626  -0.456578   \n",
       "10917   0.023335   0.013198  -0.008005  -0.012395   0.008797   0.005418   \n",
       "10918   0.034519  -0.001241  -0.061202  -0.021307   0.023097   0.009323   \n",
       "10919   0.048776   0.088065   0.111590   0.075649   0.019448   0.054759   \n",
       "\n",
       "       accel-x-6  accel-x-7  accel-x-8  accel-x-9  ...  gyro-z-50  gyro-z-51  \\\n",
       "0      -3.078670  -6.051075  -3.491006  -1.464815  ...  -0.776451  -0.576500   \n",
       "1       0.007363  -3.121700   2.422174  -2.430532  ...  -0.143452  -0.646708   \n",
       "2       4.361573   3.237100   3.697426   3.434555  ...  -1.438400  -1.782263   \n",
       "3       4.269343   3.831115   0.655725  -1.144818  ...   0.189591   0.444646   \n",
       "4       9.061282   2.349534 -10.563099  -8.488220  ...  -1.874475   0.041224   \n",
       "...          ...        ...        ...        ...  ...        ...        ...   \n",
       "10915  -0.019827  -0.037937  -0.048616  -0.003029  ...   0.007244   0.002286   \n",
       "10916  -0.295231  -0.161805  -1.112699  -0.444722  ...   0.007144   0.007289   \n",
       "10917   0.006365   0.030876   0.032646   0.032760  ...  -0.015202  -0.002399   \n",
       "10918   0.012766   0.045946  -0.063485  -0.030942  ...  -0.001505   0.000898   \n",
       "10919   0.160495   0.079276  -0.195114  -0.303471  ...  -0.002672  -0.000755   \n",
       "\n",
       "       gyro-z-52  gyro-z-53  gyro-z-54  gyro-z-55  gyro-z-56  gyro-z-57  \\\n",
       "0      -0.575899  -0.799183  -0.084052   0.752021   1.587773   2.650263   \n",
       "1       0.207145   0.702881   0.010870   0.190366  -0.486779  -0.079197   \n",
       "2      -1.537537  -1.536583  -1.961644  -1.557289   0.286954   4.903331   \n",
       "3       0.451423   0.160605  -0.055797  -0.008116   0.130876   0.558405   \n",
       "4       0.816235   0.905279   0.424946  -1.133572  -1.697517  -1.766260   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "10915  -0.004577  -0.002461  -0.002443  -0.001429   0.003391  -0.000314   \n",
       "10916  -0.001360  -0.006746   0.002004   0.000193   0.000345  -0.003875   \n",
       "10917   0.012727   0.028815   0.037020   0.045893   0.046907   0.045081   \n",
       "10918   0.000791  -0.013288  -0.003919  -0.007930  -0.002467  -0.001422   \n",
       "10919   0.000164   0.002220  -0.000085  -0.004062  -0.007611  -0.008637   \n",
       "\n",
       "       gyro-z-58  gyro-z-59  \n",
       "0       1.906534   0.092092  \n",
       "1       0.471224  -0.197064  \n",
       "2       5.336706   3.811437  \n",
       "3       0.738815   0.314354  \n",
       "4      -2.217915  -2.309474  \n",
       "...          ...        ...  \n",
       "10915   0.004041   0.006878  \n",
       "10916  -0.004737  -0.000545  \n",
       "10917   0.030555  -0.007662  \n",
       "10918  -0.004019  -0.002352  \n",
       "10919  -0.006806  -0.010109  \n",
       "\n",
       "[10920 rows x 360 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 shape: torch.Size([100, 6, 60])\n",
      "Batch 1 shape: torch.Size([100, 6, 60])\n",
      "Batch 2 shape: torch.Size([100, 6, 60])\n",
      "Batch 3 shape: torch.Size([100, 6, 60])\n",
      "Batch 4 shape: torch.Size([100, 6, 60])\n",
      "Batch 5 shape: torch.Size([100, 6, 60])\n",
      "Batch 6 shape: torch.Size([100, 6, 60])\n",
      "Batch 7 shape: torch.Size([100, 6, 60])\n",
      "Batch 8 shape: torch.Size([100, 6, 60])\n",
      "Batch 9 shape: torch.Size([100, 6, 60])\n",
      "Batch 10 shape: torch.Size([100, 6, 60])\n",
      "Batch 11 shape: torch.Size([100, 6, 60])\n",
      "Batch 12 shape: torch.Size([100, 6, 60])\n",
      "Batch 13 shape: torch.Size([100, 6, 60])\n",
      "Batch 14 shape: torch.Size([100, 6, 60])\n",
      "Batch 15 shape: torch.Size([100, 6, 60])\n",
      "Batch 16 shape: torch.Size([100, 6, 60])\n",
      "Batch 17 shape: torch.Size([100, 6, 60])\n",
      "Batch 18 shape: torch.Size([100, 6, 60])\n",
      "Batch 19 shape: torch.Size([100, 6, 60])\n",
      "Batch 20 shape: torch.Size([100, 6, 60])\n",
      "Batch 21 shape: torch.Size([100, 6, 60])\n",
      "Batch 22 shape: torch.Size([100, 6, 60])\n",
      "Batch 23 shape: torch.Size([100, 6, 60])\n",
      "Batch 24 shape: torch.Size([100, 6, 60])\n",
      "Batch 25 shape: torch.Size([100, 6, 60])\n",
      "Batch 26 shape: torch.Size([100, 6, 60])\n",
      "Batch 27 shape: torch.Size([100, 6, 60])\n",
      "Batch 28 shape: torch.Size([100, 6, 60])\n",
      "Batch 29 shape: torch.Size([100, 6, 60])\n",
      "Batch 30 shape: torch.Size([100, 6, 60])\n",
      "Batch 31 shape: torch.Size([100, 6, 60])\n",
      "Batch 32 shape: torch.Size([100, 6, 60])\n",
      "Batch 33 shape: torch.Size([100, 6, 60])\n",
      "Batch 34 shape: torch.Size([100, 6, 60])\n",
      "Batch 35 shape: torch.Size([100, 6, 60])\n",
      "Batch 36 shape: torch.Size([100, 6, 60])\n",
      "Batch 37 shape: torch.Size([100, 6, 60])\n",
      "Batch 38 shape: torch.Size([100, 6, 60])\n",
      "Batch 39 shape: torch.Size([100, 6, 60])\n",
      "Batch 40 shape: torch.Size([100, 6, 60])\n",
      "Batch 41 shape: torch.Size([100, 6, 60])\n",
      "Batch 42 shape: torch.Size([100, 6, 60])\n",
      "Batch 43 shape: torch.Size([100, 6, 60])\n",
      "Batch 44 shape: torch.Size([100, 6, 60])\n",
      "Batch 45 shape: torch.Size([100, 6, 60])\n",
      "Batch 46 shape: torch.Size([100, 6, 60])\n",
      "Batch 47 shape: torch.Size([100, 6, 60])\n",
      "Batch 48 shape: torch.Size([100, 6, 60])\n",
      "Batch 49 shape: torch.Size([100, 6, 60])\n",
      "Batch 50 shape: torch.Size([100, 6, 60])\n",
      "Batch 51 shape: torch.Size([100, 6, 60])\n",
      "Batch 52 shape: torch.Size([100, 6, 60])\n",
      "Batch 53 shape: torch.Size([100, 6, 60])\n",
      "Batch 54 shape: torch.Size([100, 6, 60])\n",
      "Batch 55 shape: torch.Size([100, 6, 60])\n",
      "Batch 56 shape: torch.Size([100, 6, 60])\n",
      "Batch 57 shape: torch.Size([100, 6, 60])\n",
      "Batch 58 shape: torch.Size([100, 6, 60])\n",
      "Batch 59 shape: torch.Size([100, 6, 60])\n",
      "Batch 60 shape: torch.Size([100, 6, 60])\n",
      "Batch 61 shape: torch.Size([100, 6, 60])\n",
      "Batch 62 shape: torch.Size([100, 6, 60])\n",
      "Batch 63 shape: torch.Size([100, 6, 60])\n",
      "Batch 64 shape: torch.Size([100, 6, 60])\n",
      "Batch 65 shape: torch.Size([100, 6, 60])\n",
      "Batch 66 shape: torch.Size([100, 6, 60])\n",
      "Batch 67 shape: torch.Size([100, 6, 60])\n",
      "Batch 68 shape: torch.Size([100, 6, 60])\n",
      "Batch 69 shape: torch.Size([100, 6, 60])\n",
      "Batch 70 shape: torch.Size([100, 6, 60])\n",
      "Batch 71 shape: torch.Size([100, 6, 60])\n",
      "Batch 72 shape: torch.Size([100, 6, 60])\n",
      "Batch 73 shape: torch.Size([100, 6, 60])\n",
      "Batch 74 shape: torch.Size([100, 6, 60])\n",
      "Batch 75 shape: torch.Size([100, 6, 60])\n",
      "Batch 76 shape: torch.Size([100, 6, 60])\n",
      "Batch 77 shape: torch.Size([100, 6, 60])\n",
      "Batch 78 shape: torch.Size([100, 6, 60])\n",
      "Batch 79 shape: torch.Size([100, 6, 60])\n",
      "Batch 80 shape: torch.Size([100, 6, 60])\n",
      "Batch 81 shape: torch.Size([100, 6, 60])\n",
      "Batch 82 shape: torch.Size([100, 6, 60])\n",
      "Batch 83 shape: torch.Size([100, 6, 60])\n",
      "Batch 84 shape: torch.Size([100, 6, 60])\n",
      "Batch 85 shape: torch.Size([100, 6, 60])\n",
      "Batch 86 shape: torch.Size([100, 6, 60])\n",
      "Batch 87 shape: torch.Size([100, 6, 60])\n",
      "Batch 88 shape: torch.Size([100, 6, 60])\n",
      "Batch 89 shape: torch.Size([100, 6, 60])\n",
      "Batch 90 shape: torch.Size([100, 6, 60])\n",
      "Batch 91 shape: torch.Size([100, 6, 60])\n",
      "Batch 92 shape: torch.Size([100, 6, 60])\n",
      "Batch 93 shape: torch.Size([100, 6, 60])\n",
      "Batch 94 shape: torch.Size([100, 6, 60])\n",
      "Batch 95 shape: torch.Size([100, 6, 60])\n",
      "Batch 96 shape: torch.Size([100, 6, 60])\n",
      "Batch 97 shape: torch.Size([100, 6, 60])\n",
      "Batch 98 shape: torch.Size([100, 6, 60])\n",
      "Batch 99 shape: torch.Size([100, 6, 60])\n",
      "Batch 100 shape: torch.Size([100, 6, 60])\n",
      "Batch 101 shape: torch.Size([100, 6, 60])\n",
      "Batch 102 shape: torch.Size([100, 6, 60])\n",
      "Batch 103 shape: torch.Size([100, 6, 60])\n",
      "Batch 104 shape: torch.Size([100, 6, 60])\n",
      "Batch 105 shape: torch.Size([100, 6, 60])\n",
      "Batch 106 shape: torch.Size([100, 6, 60])\n",
      "Batch 107 shape: torch.Size([100, 6, 60])\n",
      "Batch 108 shape: torch.Size([100, 6, 60])\n",
      "Batch 0 shape: torch.Size([100])\n",
      "Batch 1 shape: torch.Size([100])\n",
      "Batch 2 shape: torch.Size([100])\n",
      "Batch 3 shape: torch.Size([100])\n",
      "Batch 4 shape: torch.Size([100])\n",
      "Batch 5 shape: torch.Size([100])\n",
      "Batch 6 shape: torch.Size([100])\n",
      "Batch 7 shape: torch.Size([100])\n",
      "Batch 8 shape: torch.Size([100])\n",
      "Batch 9 shape: torch.Size([100])\n",
      "Batch 10 shape: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# shape dataloader\n",
    "\n",
    "for i, (x, y) in enumerate(dataloader_train):\n",
    "    print(f\"Batch {i} shape: {x.shape}\")\n",
    "\n",
    "for i, (x, y) in enumerate(dataloader_val):\n",
    "    print(f\"Batch {i} shape: {y.shape}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_size = 256\n",
    "n_classes = 7\n",
    "\n",
    "classifier = StateClassifier(input_size=encoding_size, n_classes=n_classes)\n",
    "cpc_classifier = CPC_Classifier(encoder, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1000, accelerator=\"gpu\", devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | encoder       | GRUEncoder       | 116 K \n",
      "1 | classifier    | StateClassifier  | 2.3 K \n",
      "2 | loss_function | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "118 K     Trainable params\n",
      "0         Non-trainable params\n",
      "118 K     Total params\n",
      "0.474     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  14%|█▍        | 15/109 [00:00<00:00, 96.64it/s, v_num=137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 109/109 [00:01<00:00, 65.59it/s, v_num=137, val_loss=0.648, train_loss=0.727] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 109/109 [00:01<00:00, 62.77it/s, v_num=137, val_loss=0.648, train_loss=0.727]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(cpc_classifier, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/vscode/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 32/32 [00:00<00:00, 132.84it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.6371874809265137\n",
      "        test_loss           0.7074582576751709\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.6371874809265137, 'test_loss': 0.7074582576751709}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(cpc_classifier, dataloader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
