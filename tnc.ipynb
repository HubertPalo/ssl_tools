{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import math\n",
    "import functools\n",
    "\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TNC SSL Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21, 561, 281), (21, 281), (9, 561, 288), (9, 288))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path('data/TNC/HAR_data')\n",
    "x_train = np.load(data_path/'x_train.npy')\n",
    "y_train = np.load(data_path/'y_train.npy')\n",
    "x_test = np.load(data_path/'x_test.npy')\n",
    "y_test = np.load(data_path/'y_test.npy')\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNCDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: np.ndarray,\n",
    "        mc_sample_size: int,\n",
    "        window_size: int,\n",
    "        state: float = None,\n",
    "        adf: bool = True,\n",
    "        augmentation: int = 1  # Simple repeat the vecvor 'augmentation' times\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.time_series = data\n",
    "        self.T = data.shape[-1]\n",
    "        self.window_size = window_size\n",
    "        self.sliding_gap = int(window_size * 25.2)\n",
    "        self.window_per_sample = (self.T - 2 * self.window_size) // self.sliding_gap\n",
    "        self.mc_sample_size = mc_sample_size\n",
    "        self.state = state\n",
    "        self.adf = adf\n",
    "        self.epsilon = None\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.time_series) * self.augmentation\n",
    "\n",
    "    # @functools.cache\n",
    "    def __getitem__(self, ind):\n",
    "        ind = ind % len(self.time_series)       # To repeat augmentation\n",
    "        t = np.random.randint(2 * self.window_size, self.T - 2 * self.window_size)\n",
    "        x_t = self.time_series[ind][\n",
    "            :, t - self.window_size // 2 : t + self.window_size // 2\n",
    "        ]\n",
    "        X_close = self._find_neighours(self.time_series[ind], t)\n",
    "        X_distant = self._find_non_neighours(self.time_series[ind], t)\n",
    "\n",
    "        if self.state is None:\n",
    "            y_t = -1\n",
    "        else:\n",
    "            y_t = np.round(\n",
    "                np.mean(\n",
    "                    self.state[ind][\n",
    "                        t - self.window_size // 2 : t + self.window_size // 2\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        return (\n",
    "            x_t.astype(np.float32),\n",
    "            X_close.astype(np.float32),\n",
    "            X_distant.astype(np.float32),\n",
    "            y_t,\n",
    "        )\n",
    "\n",
    "    def _find_neighours(self, x, t):\n",
    "        T = self.time_series.shape[-1]\n",
    "\n",
    "        # ---- Do the ADF test ----\n",
    "        gap = self.window_size\n",
    "        corr = []\n",
    "        for w_t in range(self.window_size, 4 * self.window_size, gap):\n",
    "            try:\n",
    "                p_val = 0\n",
    "                for f in range(x.shape[-2]):\n",
    "                    p = adfuller(\n",
    "                        np.array(\n",
    "                            x[\n",
    "                                f, max(0, t - w_t) : min(x.shape[-1], t + w_t)\n",
    "                            ].reshape(\n",
    "                                -1,\n",
    "                            )\n",
    "                        )\n",
    "                    )[1]\n",
    "                    p_val += 0.01 if math.isnan(p) else p\n",
    "                corr.append(p_val / x.shape[-2])\n",
    "            except:\n",
    "                corr.append(0.6)\n",
    "        self.epsilon = (\n",
    "            len(corr)\n",
    "            if len(np.where(np.array(corr) >= 0.01)[0]) == 0\n",
    "            else (np.where(np.array(corr) >= 0.01)[0][0] + 1)\n",
    "        )\n",
    "        self.delta = 5 * self.epsilon * self.window_size\n",
    "        # --------------------------\n",
    "\n",
    "        ## Random from a Gaussian\n",
    "        t_p = [\n",
    "            int(t + np.random.randn() * self.epsilon * self.window_size)\n",
    "            for _ in range(self.mc_sample_size)\n",
    "        ]\n",
    "        t_p = [\n",
    "            max(self.window_size // 2 + 1, min(t_pp, T - self.window_size // 2))\n",
    "            for t_pp in t_p\n",
    "        ]\n",
    "        x_p = np.stack(\n",
    "            [\n",
    "                x[:, t_ind - self.window_size // 2 : t_ind + self.window_size // 2]\n",
    "                for t_ind in t_p\n",
    "            ]\n",
    "        )\n",
    "        return x_p\n",
    "\n",
    "    def _find_non_neighours(self, x, t):\n",
    "        T = self.time_series.shape[-1]\n",
    "        if t > T / 2:\n",
    "            t_n = np.random.randint(\n",
    "                self.window_size // 2,\n",
    "                max((t - self.delta + 1), self.window_size // 2 + 1),\n",
    "                self.mc_sample_size,\n",
    "            )\n",
    "        else:\n",
    "            t_n = np.random.randint(\n",
    "                min((t + self.delta), (T - self.window_size - 1)),\n",
    "                (T - self.window_size // 2),\n",
    "                self.mc_sample_size,\n",
    "            )\n",
    "        x_n = np.stack(\n",
    "            [\n",
    "                x[:, t_ind - self.window_size // 2 : t_ind + self.window_size // 2]\n",
    "                for t_ind in t_n\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if len(x_n) == 0:\n",
    "            rand_t = np.random.randint(0, self.window_size // 5)\n",
    "            if t > T / 2:\n",
    "                x_n = x[:, rand_t : rand_t + self.window_size].unsqueeze(0)\n",
    "            else:\n",
    "                x_n = x[:, T - rand_t - self.window_size : T - rand_t].unsqueeze(0)\n",
    "        return x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, input_size, device: str = \"cpu\"):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * self.input_size, 4 * self.input_size),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(4 * self.input_size, 1),\n",
    "        ).to(device)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.model[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(self.model[3].weight)\n",
    "\n",
    "    def forward(self, x, x_tild):\n",
    "        \"\"\"\n",
    "        Predict the probability of the two inputs belonging to the same neighbourhood.\n",
    "        \"\"\"\n",
    "        x_all = torch.cat([x, x_tild], -1)\n",
    "        p = self.model(x_all)\n",
    "        return p.view((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enconder\n",
    "\n",
    "The code can be made considerably simpler when implementing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 100,\n",
    "        in_channel: int = 561,\n",
    "        encoding_size: int = 10,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        bidirectional: bool = True,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_channel = in_channel\n",
    "        self.num_layers = num_layers\n",
    "        self.encoding_size = encoding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.device = device\n",
    "        \n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=self.in_channel,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "        ).to(device)\n",
    "\n",
    "        self.nn = torch.nn.Linear(\n",
    "            self.hidden_size * (int(self.bidirectional) + 1), self.encoding_size\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)\n",
    "\n",
    "        past = torch.zeros(\n",
    "            self.num_layers * (int(self.bidirectional) + 1),\n",
    "            x.shape[1],\n",
    "            self.hidden_size,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        out, _ = self.rnn(\n",
    "            x, past\n",
    "        )  # out shape = [seq_len, batch_size, num_directions*hidden_size]\n",
    "        encodings = self.nn(out[-1].squeeze(0))\n",
    "        return encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TNC Dataset\n",
    "\n",
    "* Augmented Dickey-Fuller test:\n",
    "    The neighborhood of a window should represent similar samples, so the range (represented by $\\eta$) needs to identify the approximate time span within which the signal remains stationary, and the generative process does not change. For this purpose, Augmented Dickey-Fuller (ADF) is used to determine this region for every window.\n",
    "\n",
    "    Determining a proper value for $\\eta$ is part of the TNC framework.\n",
    "    \n",
    "    Video explaining Augmented Dickey-Fuller test: https://www.youtube.com/watch?v=1opjnegd_hA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNC(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: torch.nn.Module,\n",
    "        discriminator: torch.nn.Module,\n",
    "        mc_sample_size: int = 20,\n",
    "        window_size: int = 4,\n",
    "        w: float = 0.05,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 1e-5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.discriminator = discriminator.to(self.device)\n",
    "        self.mc_sample_size = mc_sample_size\n",
    "        self.window_size = window_size\n",
    "        self.w = w\n",
    "        self.learning_rate = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "        self.training_step_losses = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_t, x_p, x_n, _ = batch\n",
    "        mc_sample = x_p.shape[1]\n",
    "        batch_size, f_size, len_size = x_t.shape\n",
    "        x_p = x_p.view(-1, f_size, len_size)\n",
    "        x_n = x_n.view(-1, f_size, len_size)\n",
    "        x_t = x_t.repeat(mc_sample, 1, 1)\n",
    "        neighbors = torch.ones(len(x_p)).to(self.device)\n",
    "        non_neighbors = torch.zeros(len(x_n)).to(self.device)\n",
    "        x_t, x_p, x_n = x_t.to(self.device), x_p.to(self.device), x_n.to(self.device)\n",
    "\n",
    "        z_t = self.encoder(x_t)\n",
    "        z_p = self.encoder(x_p)\n",
    "        z_n = self.encoder(x_n)\n",
    "\n",
    "        d_p = self.discriminator(z_t, z_p)\n",
    "        d_n = self.discriminator(z_t, z_n)\n",
    "\n",
    "        p_loss = self.loss_func(d_p, neighbors)\n",
    "        n_loss = self.loss_func(d_n, non_neighbors)\n",
    "        n_loss_u = self.loss_func(d_n, neighbors)\n",
    "        loss = (p_loss + self.w * n_loss_u + (1 - self.w) * n_loss) / 2\n",
    "\n",
    "        self.training_step_losses.append(loss)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.training_step_losses).mean()\n",
    "        self.log(\"train_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        # free up the memory\n",
    "        self.training_step_losses.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        learnable_parameters = list(self.discriminator.parameters()) + list(\n",
    "            self.encoder.parameters()\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            learnable_parameters, lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset:\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx].astype(np.float32), self.y[idx].astype(np.float32)\n",
    "        else:\n",
    "            return self.X[idx].astype(np.float32)\n",
    "        \n",
    "# train_dataset = SimpleDataset(x_train)\n",
    "# test_dataset = SimpleDataset(x_test, y_test)\n",
    "# len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_size = 10\n",
    "window_size = 4\n",
    "augmentation = 5\n",
    "batch_size = 10\n",
    "mc_sample_size = 20\n",
    "workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 25)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test split X_train and y_train to train and validation using sklearn train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TNCDataset(X_train, mc_sample_size=20, window_size=4, augmentation=augmentation)\n",
    "validation_dataset = TNCDataset(X_val, mc_sample_size=20, window_size=4, augmentation=augmentation)\n",
    "\n",
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=workers, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, num_workers=workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the model\n",
    "discriminator = Discriminator(input_size=encoding_size, device='cuda')\n",
    "encoder = GRUEncoder(encoding_size=encoding_size, device='cuda')\n",
    "tnc_model = TNC(\n",
    "    encoder=encoder,\n",
    "    discriminator=discriminator,\n",
    "    window_size=window_size,\n",
    "    mc_sample_size=mc_sample_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=150, accelerator=\"gpu\", devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder       | GRUEncoder        | 399 K \n",
      "1 | discriminator | Discriminator     | 881   \n",
      "2 | loss_func     | BCEWithLogitsLoss | 0     \n",
      "----------------------------------------------------\n",
      "400 K     Trainable params\n",
      "0         Non-trainable params\n",
      "400 K     Total params\n",
      "1.603     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|██████████| 8/8 [00:22<00:00,  2.80s/it, v_num=24, train_loss=0.690]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=150` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|██████████| 8/8 [00:22<00:00,  2.83s/it, v_num=24, train_loss=0.690]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(tnc_model, train_dataloader, validation_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TNC Fine-tunning\n",
    "\n",
    "We are going to use the TNC model on the downstream task of classification. We will use same dataset and re-use the same encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "\n",
    "class StateClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size: int = 10, n_classes: int = 6):\n",
    "        super(StateClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.n_classes = n_classes\n",
    "        self.normalize = torch.nn.BatchNorm1d(self.input_size)\n",
    "        self.nn = torch.nn.Linear(self.input_size, self.n_classes)\n",
    "        torch.nn.init.xavier_uniform_(self.nn.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.normalize(x)\n",
    "        logits = self.nn(x)\n",
    "        return logits\n",
    "    \n",
    "class TNC_Classifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: torch.nn.Module,\n",
    "        classifier: torch.nn.Module,\n",
    "        lr: float = 1e-2,\n",
    "        weight_decay: float = 0.0,\n",
    "        task_class: str = \"multiclass\",\n",
    "        num_classes: int = 6\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.classifier = classifier.to(self.device)\n",
    "        self.learning_rate = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.training_step_losses = []\n",
    "        self.validation_step_losses = []\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        self.task_class = task_class\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def configure_optimizers(self) -> Any:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.classifier.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encodings = self.encoder(x)\n",
    "        predictions = self.classifier(encodings)\n",
    "        return predictions\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        predictions = self.forward(x)\n",
    "        loss = self.loss_function(predictions, y.long())\n",
    "        self.training_step_losses.append(loss)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.training_step_losses).mean()\n",
    "        self.log(\"train_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        # free up the memory\n",
    "        self.training_step_losses.clear()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        self.validation_step_losses.append(loss)\n",
    "        metrics = {\"val_acc\": acc, \"val_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "        \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        # do something with all training_step outputs, for example:\n",
    "        epoch_mean = torch.stack(self.validation_step_losses).mean()\n",
    "        self.log(\"val_loss\", epoch_mean, on_epoch=True, on_step=False, prog_bar=True, logger=True)\n",
    "        # free up the memory\n",
    "        self.validation_step_losses.clear()\n",
    "\n",
    "    def _shared_eval_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        predictions = self.forward(x)\n",
    "        loss = self.loss_function(predictions, y.long())\n",
    "        acc = accuracy(torch.argmax(predictions, dim=1), y.long(), task=self.task_class, num_classes=self.num_classes)\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21, 561, 281), (21, 281), (9, 561, 288), (9, 288))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path('data/TNC/HAR_data')\n",
    "x_train = np.load(data_path/'x_train.npy')\n",
    "y_train = np.load(data_path/'y_train.npy')\n",
    "x_test = np.load(data_path/'x_test.npy')\n",
    "y_test = np.load(data_path/'y_test.npy')\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code as taken from the authors\n",
    "\n",
    "def create_simulated_dataset(\n",
    "    X_train, Y_train, X_test, Y_test, window_size=50, batch_size=100\n",
    "):\n",
    "    n_train = int(0.8 * len(X_train))\n",
    "    n_valid = len(X_train) - n_train\n",
    "    n_test = len(X_test)\n",
    "    x_train, y_train = X_train[:n_train], Y_train[:n_train]\n",
    "    x_valid, y_valid = X_train[n_train:], Y_train[n_train:]\n",
    "    x_test, y_test = X_test, Y_test\n",
    "\n",
    "    datasets = []\n",
    "    for x, y, size in [\n",
    "        (x_train, y_train, n_train),\n",
    "        (x_test, y_test, n_test),\n",
    "        (x_valid, y_valid, n_valid),\n",
    "    ]:\n",
    "        T = x.shape[-1]\n",
    "        windows = np.split(\n",
    "            x[:, :, : window_size * (T // window_size)], (T // window_size), -1\n",
    "        )\n",
    "        windows = np.concatenate(windows, 0)\n",
    "        labels = np.split(\n",
    "            y[:, : window_size * (T // window_size)], (T // window_size), -1\n",
    "        )\n",
    "        labels = np.round(np.mean(np.concatenate(labels, 0), -1))\n",
    "        dset = SimpleDataset(windows, labels)\n",
    "        datasets.append(dset)\n",
    "\n",
    "    trainset, testset, validset = datasets[0], datasets[1], datasets[2]\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(validset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = create_simulated_dataset(x_train, y_train, x_test, y_test, window_size=4, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_size = 10\n",
    "n_classes = 6\n",
    "\n",
    "classifier = StateClassifier(input_size=encoding_size, n_classes=n_classes)\n",
    "tnc_classifier = TNC_Classifier(encoder, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=50, accelerator=\"gpu\", devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | encoder       | GRUEncoder       | 399 K \n",
      "1 | classifier    | StateClassifier  | 86    \n",
      "2 | loss_function | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------\n",
      "399 K     Trainable params\n",
      "0         Non-trainable params\n",
      "399 K     Total params\n",
      "1.600     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   8%|▊         | 1/12 [00:00<00:00, 35.18it/s, v_num=48]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 12/12 [00:00<00:00, 19.40it/s, v_num=48, val_loss=0.958, train_loss=1.020]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 12/12 [00:00<00:00, 13.59it/s, v_num=48, val_loss=0.958, train_loss=1.020]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(tnc_classifier, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 39.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.49074074625968933    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9984065890312195     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.49074074625968933   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9984065890312195    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.49074074625968933, 'test_loss': 0.9984065890312195}]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(tnc_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
